diff --git a/pom.xml b/pom.xml
index 06908518656f6c32b9a1105e1684bf9102550d1b..0a3c319fef62ddc7a97a982c552a8712b68da397 100644
--- a/pom.xml
+++ b/pom.xml
@@ -48,22 +48,35 @@
     </pluginRepositories>
 
     <dependencies>
+    	<dependency>
+            <groupId>com.google.guava</groupId>
+            <artifactId>guava</artifactId>
+	    <version>16.0.1</version>
+        </dependency>
         <dependency>
             <groupId>net.java.dev.jets3t</groupId>
             <artifactId>jets3t</artifactId>
             <version>0.7.1</version>
         </dependency>
+
+        <dependency>
+            <groupId>org.apache.kafka</groupId>
+            <artifactId>kafka-clients</artifactId>
+            <version>0.9.0.1</version>
+        </dependency>
+
         <dependency>
             <groupId>org.apache.kafka</groupId>
             <artifactId>kafka_2.10</artifactId>
-            <version>0.8.1.1</version>
-            <exclusions>
-                <exclusion>
-                    <groupId>org.slf4j</groupId>
-                    <artifactId>slf4j-simple</artifactId>
-                </exclusion>
-            </exclusions>
+            <version>0.9.0.1</version>
         </dependency>
+
+        <dependency>
+            <groupId>com.ibm.stocator</groupId>
+            <artifactId>stocator</artifactId>
+            <version>1.0.4</version>
+        </dependency>
+
         <dependency>
             <groupId>log4j</groupId>
             <artifactId>log4j</artifactId>
@@ -104,20 +117,29 @@
             <version>1.9</version>
         </dependency>
         <dependency>
+            <groupId>org.json</groupId>
+            <artifactId>json</artifactId>
+            <version>20140107</version>
+        </dependency>
+        <dependency>
             <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-core</artifactId>
-            <version>1.2.1</version>
-            <exclusions>
-                <exclusion>
-                    <groupId>net.java.dev.jets3t</groupId>
-                    <artifactId>jets3t</artifactId>
-                </exclusion>
-            </exclusions>
+            <artifactId>hadoop-client</artifactId>
+            <version>2.6.0</version>
         </dependency>
         <dependency>
-            <groupId>org.apache.thrift</groupId>
+            <groupId>com.twitter</groupId>
+            <artifactId>parquet-avro</artifactId>
+            <version>1.5.0</version>
+        </dependency>
+		<dependency>
+			<groupId>com.twitter</groupId>
+			<artifactId>parquet-hadoop</artifactId>
+			<version>1.5.0</version>
+		</dependency>
+        <dependency>
+          <groupId>org.apache.thrift</groupId>
             <artifactId>libthrift</artifactId>
-            <version>0.5.0</version>
+            <version>0.9.2</version>
         </dependency>
         <dependency>
             <groupId>com.twitter</groupId>
@@ -127,17 +149,12 @@
         <dependency>
             <groupId>com.twitter.common.zookeeper</groupId>
             <artifactId>lock</artifactId>
-            <version>0.0.7</version>
-        </dependency>
-        <dependency>
-            <groupId>com.google.guava</groupId>
-            <artifactId>guava</artifactId>
-            <version>18.0</version>
+            <version>0.0.40</version>
         </dependency>
         <dependency>
             <groupId>net.minidev</groupId>
             <artifactId>json-smart</artifactId>
-            <version>2.0-RC3</version>
+            <version>2.2.1</version>
         </dependency>
         <dependency>
             <groupId>junit</groupId>
@@ -170,7 +187,17 @@
         <dependency>
             <groupId>com.timgroup</groupId>
             <artifactId>java-statsd-client</artifactId>
-            <version>3.0.2</version>
+            <version>3.1.0</version>
+        </dependency>
+        <dependency>
+            <groupId>org.javaswift</groupId>
+            <artifactId>joss</artifactId>
+            <version>0.9.8</version>
+        </dependency>
+        <dependency>
+            <groupId>commons-codec</groupId>
+            <artifactId>commons-codec</artifactId>
+            <version>1.10</version>
         </dependency>
     </dependencies>
 
@@ -240,7 +267,30 @@
                     </execution>
                 </executions>
             </plugin>
+
+            <!-- This create the code coverage report -->
             <plugin>
+                <groupId>org.jacoco</groupId>
+                <artifactId>jacoco-maven-plugin</artifactId>
+                <version>0.7.4.201502262128</version>
+                <executions>
+                    <execution>
+                        <id>default-prepare-agent</id>
+                        <goals>
+                            <goal>prepare-agent</goal>
+                        </goals>
+                    </execution>
+                    <execution>
+                        <id>default-report</id>
+                        <phase>prepare-package</phase>
+                        <goals>
+                            <goal>report</goal>
+                        </goals>
+                    </execution>
+                </executions>
+            </plugin>
+
+            <!-- <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-assembly-plugin</artifactId>
                 <executions>
@@ -257,7 +307,7 @@
                         </configuration>
                     </execution>
                 </executions>
-            </plugin>
+            </plugin> -->
             <!-- Thrift source generate plugin. -->
             <plugin>
                 <groupId>com.twitter</groupId>
diff --git a/src/main/config/log4j.prod.properties b/src/main/config/log4j.prod.properties
index 298e2cdb99fa771cfc45adf40b905676da68306c..b162e43b80bde1439d4b571f3c497cdef668a7df 100644
--- a/src/main/config/log4j.prod.properties
+++ b/src/main/config/log4j.prod.properties
@@ -4,7 +4,7 @@
 log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE
 
 log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
-log4j.appender.CONSOLE.Threshold=WARN
+log4j.appender.CONSOLE.Threshold=INFO
 log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
 log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%C:%L) %-5p %m%n
 
diff --git a/src/main/config/secor.common.properties b/src/main/config/secor.common.properties
index 3250bef62a332a6b729629b4a06a0dd452a4773f..b21bea29ba35dc6403f6f6c181839b9a0a51b1f8 100644
--- a/src/main/config/secor.common.properties
+++ b/src/main/config/secor.common.properties
@@ -20,10 +20,35 @@
 # Regular expression matching names of consumed topics.
 secor.kafka.topic_filter=.*
 
+# Choose what to fill according to the service you are using
+# in the choice option you MUST fill S3 or Swift
+
+cloud.service=Swift
+#cloud.service=S3
+
 # AWS authentication credentials.
 aws.access.key=
 aws.secret.key=
 
+####### Swift Login Details: #########
+# Determines the type of authentication (true - use tempAuth, false - use Keystone)
+swift.auth.method=keystoneV3
+# Swift authentication URL.
+swift.auth.url=https://identity.open.softlayer.com/v3/auth/tokens
+swift.tenant= <projectId>
+swift.username= <userId>
+swift.port=8080
+swift.endpoint.prefix=endpoints
+swift.region=dallas
+# Determines if the storage URL returned by authentication service can be used only in internal network or from any computer
+swift.public=true
+
+# only needed if "swift.use.get.auth" = false
+swift.password= <password>
+
+# only needed if "swift.use.get.auth" = true
+# swift.api.key=
+
 ################
 # END MUST SET #
 ################
@@ -31,6 +56,7 @@ aws.secret.key=
 # Zookeeper config.
 zookeeper.session.timeout.ms=3000
 zookeeper.sync.time.ms=200
+zookeeper.connection.timeout.ms=200
 
 # Impacts how frequently the upload logic is triggered if no messages are delivered.
 kafka.consumer.timeout.ms=10000
@@ -66,7 +92,7 @@ kafka.zookeeper.path=/
 secor.generation=1
 
 # Number of consumer threads per Secor process.
-secor.consumer.threads=7
+secor.consumer.threads=7 
 
 # Consumption rate limit enforced at the process level (not a consumer-thread level).
 secor.messages.per.second=10000
@@ -111,6 +137,12 @@ statsd.hostport=
 # Name of field that contains timestamp for JSON, MessagePack, or Thrift message parser. (1405970352123)
 message.timestamp.name=timestamp
 
+# Separator for defining message.timestamp.name in a nested structure. E.g.
+# {"meta_data": {"created": "1405911096123", "last_modified": "1405912096123"}, "data": "test"}
+# message.timestamp.name=meta_data.created
+# message.timestamp.name.separator=.
+message.timestamp.name.separator=
+
 # Name of field that contains a timestamp, as a date Format, for JSON. (2014-08-07, Jul 23 02:16:57 2005, etc...)
 # Should be used when there is no timestamp in a Long format. Also ignore time zones.
 message.timestamp.input.pattern=
@@ -121,9 +153,12 @@ message.timestamp.input.pattern=
 secor.compression.codec=
 
 # The secor file reader/writer used to read/write the data, by default we write sequence files
-secor.file.reader.writer.factory=com.pinterest.secor.io.impl.SequenceFileReaderWriterFactory
+secor.file.reader.writer.factory=com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory
 
 # Max message size in bytes to retrieve via KafkaClient. This is used by ProgressMonitor and PartitionFinalizer.
 # This should be set large enough to accept the max message size configured in your kafka broker
 # Default is 0.1 MB
 secor.max.message.size.bytes=100000
+
+
+
diff --git a/src/main/config/secor.dev.backup.properties b/src/main/config/secor.dev.backup.properties
index fac8845d5ba7032afe936d53ec31287a096d4d91..c03b72ce02aa12d7261620f207194de17315b339 100644
--- a/src/main/config/secor.dev.backup.properties
+++ b/src/main/config/secor.dev.backup.properties
@@ -24,6 +24,9 @@ secor.message.parser.class=com.pinterest.secor.parser.OffsetMessageParser
 # S3 path where sequence files are stored.
 secor.s3.path=secor_dev/backup
 
+# Swift path where sequence files are stored.
+secor.swift.path=secorDev_backup
+
 # Local path where sequence files are stored before they are uploaded to s3.
 secor.local.path=/tmp/secor_dev/message_logs/backup
 
diff --git a/src/main/config/secor.dev.partition.properties b/src/main/config/secor.dev.partition.properties
index d029c86d014e3ab41c08ac57f27771652e41979d..3c70da8deba42a437f25a55c1d2bf897f9608f84 100644
--- a/src/main/config/secor.dev.partition.properties
+++ b/src/main/config/secor.dev.partition.properties
@@ -24,6 +24,9 @@ secor.message.parser.class=com.pinterest.secor.parser.ThriftMessageParser
 # S3 path where sequence files are stored.
 secor.s3.path=secor_dev/partition
 
+# Swift path where sequence files are stored.
+secor.swift.path=secorDev_partition
+
 # Local path where sequence files are stored before they are uploaded to s3.
 secor.local.path=/tmp/secor_dev/message_logs/partition
 
diff --git a/src/main/config/secor.dev.properties b/src/main/config/secor.dev.properties
index b1ad32099df47c85ba058c6395d75149fbdfc532..03f730f696577f3853b3f4696cd10fe1c08fe935 100644
--- a/src/main/config/secor.dev.properties
+++ b/src/main/config/secor.dev.properties
@@ -4,9 +4,25 @@ include=secor.common.properties
 # MUST SET #
 ############
 
+# Fill the section which fits your needs
+###############
+#  Using S3   #
+###############
+
 # Name of the s3 bucket where log files are stored.
 secor.s3.bucket=
 
+###############
+# Using Swift #
+###############
+
+# Boolean variable which determines if each topic will be uploaded to different container
+# (Created automatic) - if true the next setting will be ignored
+secor.swift.containers.for.each.topic=false
+
+# Name of swift container where log files are stored.
+secor.swift.container=logsContainer
+
 ################
 # END MUST SET #
 ################
@@ -22,3 +38,21 @@ secor.max.file.size.bytes=10000
 # 1 minute
 secor.max.file.age.seconds=60
 
+# Info container for ParquetFactoryReaderWriter 
+# in this container there will be .schema files and .metaKeys files
+# The schema is in Avro format
+# the metaKeys is each key in row
+secor.info.container=secorSchema
+
+# Parquet options
+secor.parquet.block_size=16777216
+secor.parquet.page_size=65536
+
+# SSL configurations (for supporting brokers)
+kafka.security.protocol=PLAINTEXT
+kafka.ssl.protocol=
+kafka.ssl.enabled.protocols=
+kafka.ssl.truststore.location=
+kafka.ssl.truststore.password=
+kafka.ssl.truststore.type=
+kafka.ssl.endpoint.identification.algorithm=
diff --git a/src/main/config/secor.prod.backup.properties b/src/main/config/secor.prod.backup.properties
index 35f57f0e8031241bfd215df2b2006dc1215faef9..7141cdf4595f7cf4d3a7a46445f29c5ab3a2f74b 100644
--- a/src/main/config/secor.prod.backup.properties
+++ b/src/main/config/secor.prod.backup.properties
@@ -18,12 +18,15 @@ include=secor.prod.properties
 # Name of the Kafka consumer group.
 secor.kafka.group=secor_backup
 
-# Parser class that extracts s3 partitions from consumed messages.
+# Parser class that extracts partitions according to the offset.
 secor.message.parser.class=com.pinterest.secor.parser.OffsetMessageParser
 
 # S3 path where sequence files are stored.
 secor.s3.path=raw_logs/secor_backup
 
+# Swift path where sequence files are stored.
+secor.swift.path=
+
 # Local path where sequence files are stored before they are uploaded to s3.
 secor.local.path=/mnt/secor_data/message_logs/backup
 
diff --git a/src/main/config/secor.prod.partition.properties b/src/main/config/secor.prod.partition.properties
index 14e36e66714fad6a38757919d3076ff05547fb9b..5999cb830429b155373b2b6164f113f06c0af49e 100644
--- a/src/main/config/secor.prod.partition.properties
+++ b/src/main/config/secor.prod.partition.properties
@@ -19,11 +19,14 @@ include=secor.prod.properties
 secor.kafka.group=secor_partition
 
 # Parser class that extracts s3 partitions from consumed messages.
-secor.message.parser.class=com.pinterest.secor.parser.ThriftMessageParser
+secor.message.parser.class=com.pinterest.secor.parser.JsonMessageParser
 
 # S3 path where sequence files are stored.
 secor.s3.path=raw_logs
 
+# Swift path where sequence files are stored.
+secor.swift.path=
+
 # Local path where sequence files are stored before they are uploaded to s3.
 secor.local.path=/mnt/secor_data/message_logs/partition
 
diff --git a/src/main/config/secor.prod.properties b/src/main/config/secor.prod.properties
index 0801a5c99185a97a605dd220a046b9d44c126505..c1a8042f2b4e9774026bd028911f35efcc84d53b 100644
--- a/src/main/config/secor.prod.properties
+++ b/src/main/config/secor.prod.properties
@@ -19,15 +19,34 @@ include=secor.common.properties
 # MUST SET #
 ############
 
-# Name of the s3 bucket where log files are stored.
-secor.s3.bucket=
-
 # Name of one (random) Kafka broker host that is used to retrieve metadata.
 # TODO(pawel): use a list of nodes or even better, extract active brokers from zookeeper.
-kafka.seed.broker.host=
+kafka.seed.broker.host=kafka01-stage1.messagehub.services.us-south.bluemix.net:9093\,kafka02-stage1.messagehub.services.us-south.bluemix.net:9093\,kafka03-stage1.messagehub.services.us-south.bluemix.net:9093\,kafka04-stage1.messagehub.services.us-south.bluemix.net:9093\,kafka05-stage1.messagehub.services.us-south.bluemix.net:9093
+
+# Kafka client ID - not needed
+kafka.client.id=
 
 # List of Kafka Zookeeper servers.
-zookeeper.quorum=
+zookeeper.quorum=localhost:2181
+
+# Fill the section which fits your needs
+###############
+#  Using S3   #
+###############
+
+# Name of the s3 bucket where log files are stored.
+secor.s3.bucket=
+
+###############
+# Using Swift #
+###############
+
+# Boolean variable which determines if each topic will be uploaded to different container
+# (Created automatic) - if true the next setting will be ignored
+secor.swift.containers.for.each.topic=false
+
+# Name of swift container where log files are stored.
+secor.swift.container=messageHub
 
 ################
 # END MUST SET #
@@ -37,5 +56,24 @@ zookeeper.quorum=
 # 200MB
 secor.max.file.size.bytes=200000000
 # 1 hour
-secor.max.file.age.seconds=3600
+secor.max.file.age.seconds=180
+
+# Info container for ParquetFactoryReaderWriter 
+# in this container there will be .schema files and .metaKeys files
+# The schema is in Avro format
+# the metaKeys is each key in row
+secor.info.container=secorSchema
+
+# Parquet options
+secor.parquet.block_size=16777216
+secor.parquet.page_size=65536
 
+# SSL configurations (for supporting brokers)
+kafka.security.protocol=SASL_SSL
+kafka.ssl.protocol=TLSv1.2
+kafka.ssl.enabled.protocols=TLSv1.2
+kafka.ssl.truststore.location=/usr/lib/jvm/default-java/jre/lib/security/cacerts
+kafka.ssl.truststore.password=changeit
+kafka.ssl.truststore.type=JKS
+kafka.ssl.endpoint.identification.algorithm=HTTPS
+kafka.sasl.mechanism=PLAIN
\ No newline at end of file
diff --git a/src/main/java/com/pinterest/secor/common/FileRegistry.java b/src/main/java/com/pinterest/secor/common/FileRegistry.java
index ac0c054762a4b6577edc78e1d8d8eb50232e6821..e292b1464ef57eb83315c90c3bca18737fa8b24a 100644
--- a/src/main/java/com/pinterest/secor/common/FileRegistry.java
+++ b/src/main/java/com/pinterest/secor/common/FileRegistry.java
@@ -21,6 +21,8 @@ import com.pinterest.secor.util.FileUtil;
 import com.pinterest.secor.util.ReflectionUtil;
 import com.pinterest.secor.util.StatsUtil;
 
+import org.apache.kafka.common.TopicPartition;
+
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -84,7 +86,7 @@ public class FileRegistry {
      */
     public FileWriter getOrCreateWriter(LogFilePath path, CompressionCodec codec)
             throws Exception {
-        FileWriter writer = mWriters.get(path);
+    	FileWriter writer = mWriters.get(path);
         if (writer == null) {
             // Just in case.
             FileUtil.delete(path.getLogFilePath());
@@ -99,10 +101,10 @@ public class FileRegistry {
             if (!files.contains(path)) {
                 files.add(path);
             }
-            writer = ReflectionUtil.createFileWriter(mConfig.getFileReaderWriterFactory(), path, codec);
+        	writer = ReflectionUtil.createFileWriter(mConfig.getFileReaderWriterFactory(), path, codec);
             mWriters.put(path, writer);
             mCreationTimes.put(path, System.currentTimeMillis() / 1000L);
-            LOG.debug("created writer for path " + path.getLogFilePath());
+            LOG.debug("created writer for path {}", path.getLogFilePath());
         }
         return writer;
     }
@@ -119,10 +121,10 @@ public class FileRegistry {
         paths.remove(path);
         if (paths.isEmpty()) {
             mFiles.remove(topicPartition);
-            StatsUtil.clearLabel("secor.size." + topicPartition.getTopic() + "." +
-                                 topicPartition.getPartition());
-            StatsUtil.clearLabel("secor.modification_age_sec." + topicPartition.getTopic() + "." +
-                                 topicPartition.getPartition());
+            StatsUtil.clearLabel("secor.size." + topicPartition.topic() + "." +
+                                 topicPartition.partition());
+            StatsUtil.clearLabel("secor.modification_age_sec." + topicPartition.topic() + "." +
+                                 topicPartition.partition());
         }
         deleteWriter(path);
         FileUtil.delete(path.getLogFilePath());
@@ -152,9 +154,9 @@ public class FileRegistry {
     public void deleteWriter(LogFilePath path) throws IOException {
         FileWriter writer = mWriters.get(path);
         if (writer == null) {
-            LOG.warn("No writer found for path " + path.getLogFilePath());
+            LOG.warn("No writer found for path {}", path.getLogFilePath());
         } else {
-            LOG.info("Deleting writer for path " + path.getLogFilePath());
+            LOG.info("Deleting writer for path {}", path.getLogFilePath());
             writer.close();
             mWriters.remove(path);
             mCreationTimes.remove(path);
@@ -168,8 +170,8 @@ public class FileRegistry {
     public void deleteWriters(TopicPartition topicPartition) throws IOException {
         HashSet<LogFilePath> paths = mFiles.get(topicPartition);
         if (paths == null) {
-            LOG.warn("No paths found for topic " + topicPartition.getTopic() + " partition " +
-                     topicPartition.getPartition());
+            LOG.warn("No paths found for topic {} partition {}",
+                    topicPartition.topic(), topicPartition.partition());
         } else {
             for (LogFilePath path : paths) {
                 deleteWriter(path);
@@ -193,8 +195,8 @@ public class FileRegistry {
                 result += writer.getLength();
             }
         }
-        StatsUtil.setLabel("secor.size." + topicPartition.getTopic() + "." +
-                           topicPartition.getPartition(), Long.toString(result));
+        StatsUtil.setLabel("secor.size." + topicPartition.topic() + "." +
+                           topicPartition.partition(), Long.toString(result));
         return result;
     }
 
@@ -212,7 +214,7 @@ public class FileRegistry {
         for (LogFilePath path : paths) {
             Long creationTime = mCreationTimes.get(path);
             if (creationTime == null) {
-                LOG.warn("no creation time found for path " + path);
+                LOG.warn("no creation time found for path {}", path);
                 creationTime = now;
             }
             long age = now - creationTime;
@@ -223,8 +225,8 @@ public class FileRegistry {
         if (result == Long.MAX_VALUE) {
             result = -1;
         }
-        StatsUtil.setLabel("secor.modification_age_sec." + topicPartition.getTopic() + "." +
-            topicPartition.getPartition(), Long.toString(result));
+        StatsUtil.setLabel("secor.modification_age_sec." + topicPartition.topic() + "." +
+            topicPartition.partition(), Long.toString(result));
         return result;
     }
 }
diff --git a/src/main/java/com/pinterest/secor/common/KafkaClient.java b/src/main/java/com/pinterest/secor/common/KafkaClient.java
index b7edc61b126a5cafc85de58944f1267704283372..9e3fdef9fce68e07d6f8e3b57f3bd937aeb6e2b9 100644
--- a/src/main/java/com/pinterest/secor/common/KafkaClient.java
+++ b/src/main/java/com/pinterest/secor/common/KafkaClient.java
@@ -16,29 +16,23 @@
  */
 package com.pinterest.secor.common;
 
-import com.google.common.net.HostAndPort;
 import com.pinterest.secor.message.Message;
-import kafka.api.FetchRequestBuilder;
-import kafka.api.PartitionOffsetRequestInfo;
-import kafka.common.TopicAndPartition;
-import kafka.javaapi.FetchResponse;
-import kafka.javaapi.OffsetRequest;
-import kafka.javaapi.OffsetResponse;
-import kafka.javaapi.PartitionMetadata;
-import kafka.javaapi.TopicMetadata;
-import kafka.javaapi.TopicMetadataRequest;
-import kafka.javaapi.TopicMetadataResponse;
-import kafka.javaapi.consumer.SimpleConsumer;
-import kafka.message.MessageAndOffset;
+
+import com.twitter.common.zookeeper.ZooKeeperClient;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.ConsumerRecords;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
 import org.apache.thrift.TException;
+import org.apache.kafka.common.TopicPartition;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.nio.ByteBuffer;
 import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.Properties;
+import java.net.UnknownHostException;
+import java.util.concurrent.TimeoutException;
 
 /**
  * Kafka client encapsulates the logic interacting with Kafka brokers.
@@ -51,138 +45,125 @@ public class KafkaClient {
     private SecorConfig mConfig;
     private ZookeeperConnector mZookeeperConnector;
 
-    public KafkaClient(SecorConfig config) {
+    public KafkaClient(SecorConfig config) throws ZooKeeperClient.ZooKeeperConnectionException, InterruptedException, TimeoutException {
         mConfig = config;
         mZookeeperConnector = new ZookeeperConnector(mConfig);
     }
 
-    private HostAndPort findLeader(TopicPartition topicPartition) {
-        SimpleConsumer consumer = null;
-        try {
-            LOG.info("looking up leader for topic " + topicPartition.getTopic() + " partition " +
-                topicPartition.getPartition());
-            consumer = new SimpleConsumer(mConfig.getKafkaSeedBrokerHost(),
-                    mConfig.getKafkaSeedBrokerPort(),
-                    100000, 64 * 1024, "leaderLookup");
-            List<String> topics = new ArrayList<String>();
-            topics.add(topicPartition.getTopic());
-            TopicMetadataRequest request = new TopicMetadataRequest(topics);
-            TopicMetadataResponse response = consumer.send(request);
-
-            List<TopicMetadata> metaData = response.topicsMetadata();
-            for (TopicMetadata item : metaData) {
-                for (PartitionMetadata part : item.partitionsMetadata()) {
-                    if (part.partitionId() == topicPartition.getPartition()) {
-                        return HostAndPort.fromParts(part.leader().host(), part.leader().port());
-                    }
-                }
-            }
-        } finally {
-            if (consumer != null) {
-                consumer.close();
-            }
-        }
-        return null;
-    }
+    public KafkaConsumer<String, byte[]> createConsumer(TopicPartition topicPartition) throws UnknownHostException {
+        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(
+                mConfig.createPropsConfig(),
+                new StringDeserializer(),
+                new ByteArrayDeserializer());
+        ArrayList<TopicPartition> topicPartitions = new ArrayList<TopicPartition>();
+        topicPartitions.add(topicPartition);
+        consumer.assign(topicPartitions);
 
-    private static String getClientName(TopicPartition topicPartition) {
-        return "secorClient_" + topicPartition.getTopic() + "_" + topicPartition.getPartition();
+        return consumer;
     }
 
-    private long findLastOffset(TopicPartition topicPartition, SimpleConsumer consumer) {
-        TopicAndPartition topicAndPartition = new TopicAndPartition(topicPartition.getTopic(),
-                topicPartition.getPartition());
-        Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo =
-                new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
-        requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(
-                kafka.api.OffsetRequest.LatestTime(), 1));
-        final String clientName = getClientName(topicPartition);
-        OffsetRequest request = new OffsetRequest(requestInfo,
-                                                  kafka.api.OffsetRequest.CurrentVersion(),
-                                                  clientName);
-        OffsetResponse response = consumer.getOffsetsBefore(request);
-
-        if (response.hasError()) {
-            throw new RuntimeException("Error fetching offset data. Reason: " +
-                    response.errorCode(topicPartition.getTopic(), topicPartition.getPartition()));
+    private Message getMessage(TopicPartition topicPartition, long offset, 
+            KafkaConsumer<String, byte[]> consumer) {
+
+        LOG.info("fetching message topic {} partition {} offset {}",
+                topicPartition.topic(), topicPartition.partition(), offset);
+
+        long lastSeenOffset = consumer.committed(topicPartition).offset();
+        consumer.seek(topicPartition, offset);
+
+        ConsumerRecords<String, byte[]> consumerRecords = consumer.poll(1000L);
+        int i=0;
+        while (consumerRecords.isEmpty() && i<3) {
+            consumerRecords = consumer.poll(1000L);
+            i++;
         }
-        long[] offsets = response.offsets(topicPartition.getTopic(),
-                topicPartition.getPartition());
-        return offsets[0] - 1;
-    }
+        consumer.seek(topicPartition, lastSeenOffset);
+        consumer.close();
 
-    private Message getMessage(TopicPartition topicPartition, long offset,
-                               SimpleConsumer consumer) {
-        LOG.info("fetching message topic " + topicPartition.getTopic() + " partition " +
-                topicPartition.getPartition() + " offset " + offset);
-        final int MAX_MESSAGE_SIZE_BYTES = mConfig.getMaxMessageSizeBytes();
-        final String clientName = getClientName(topicPartition);
-        kafka.api.FetchRequest request = new FetchRequestBuilder().clientId(clientName)
-                .addFetch(topicPartition.getTopic(), topicPartition.getPartition(), offset,
-                          MAX_MESSAGE_SIZE_BYTES)
-                .build();
-        FetchResponse response = consumer.fetch(request);
-        if (response.hasError()) {
-            consumer.close();
-            throw new RuntimeException("Error fetching offset data. Reason: " +
-                    response.errorCode(topicPartition.getTopic(), topicPartition.getPartition()));
+        if (consumerRecords.isEmpty()) {
+            LOG.warn("consumerRecords was empty, returned null");
+            return null;
         }
-        MessageAndOffset messageAndOffset = response.messageSet(
-                topicPartition.getTopic(), topicPartition.getPartition()).iterator().next();
-        ByteBuffer payload = messageAndOffset.message().payload();
-        byte[] payloadBytes = new byte[payload.limit()];
-        payload.get(payloadBytes);
-        return new Message(topicPartition.getTopic(), topicPartition.getPartition(),
-                messageAndOffset.offset(), payloadBytes);
-    }
 
-   public SimpleConsumer createConsumer(TopicPartition topicPartition) {
-        HostAndPort leader = findLeader(topicPartition);
-        LOG.info("leader for topic " + topicPartition.getTopic() + " partition " +
-                 topicPartition.getPartition() + " is " + leader.toString());
-        final String clientName = getClientName(topicPartition);
-        return new SimpleConsumer(leader.getHostText(), leader.getPort(), 100000, 64 * 1024,
-                                  clientName);
+        ConsumerRecord<String, byte[]> consumerRecord = consumerRecords.records(topicPartition).get(0);
+
+        return new Message(consumerRecord.topic(), consumerRecord.partition(),
+                consumerRecord.offset(), consumerRecord.value());
     }
 
+
     public int getNumPartitions(String topic) {
-        SimpleConsumer consumer = null;
+        KafkaConsumer<String, byte[]> consumer = null;
         try {
-            consumer = new SimpleConsumer(mConfig.getKafkaSeedBrokerHost(),
-                    mConfig.getKafkaSeedBrokerPort(),
-                    100000, 64 * 1024, "partitionLookup");
-            List<String> topics = new ArrayList<String>();
-            topics.add(topic);
-            TopicMetadataRequest request = new TopicMetadataRequest(topics);
-            TopicMetadataResponse response = consumer.send(request);
-            if (response.topicsMetadata().size() != 1) {
-                throw new RuntimeException("Expected one metadata for topic " + topic + " found " +
-                    response.topicsMetadata().size());
-            }
-            TopicMetadata topicMetadata = response.topicsMetadata().get(0);
-            return topicMetadata.partitionsMetadata().size();
+            consumer = new KafkaConsumer<String, byte[]>(
+                    mConfig.createPropsConfig(),
+                    new StringDeserializer(),
+                    new ByteArrayDeserializer());
+
+            return consumer.partitionsFor(topic).size();
+        } catch (Exception e) {
+            e.printStackTrace();
         } finally {
             if (consumer != null) {
                 consumer.close();
             }
         }
+        return 0;
     }
 
-    public Message getLastMessage(TopicPartition topicPartition) throws TException {
-        SimpleConsumer consumer = createConsumer(topicPartition);
-        long lastOffset = findLastOffset(topicPartition, consumer);
+    public Message getLastMessage(TopicPartition topicPartition) throws TException, UnknownHostException {
+        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(
+                mConfig.createPropsConfig(),
+                new StringDeserializer(),
+                new ByteArrayDeserializer());
+
+        ArrayList<TopicPartition> topicPartitionsList = new ArrayList<TopicPartition>();
+        topicPartitionsList.add(topicPartition);
+        consumer.assign(topicPartitionsList);
+
+        consumer.seekToEnd(topicPartition);
+        long lastOffset = consumer.position(topicPartition);
+
         if (lastOffset < 1) {
             return null;
         }
-        return getMessage(topicPartition, lastOffset, consumer);
+
+        return getMessage(topicPartition, lastOffset-1, consumer);
     }
 
     public Message getCommittedMessage(TopicPartition topicPartition) throws Exception {
         long committedOffset = mZookeeperConnector.getCommittedOffsetCount(topicPartition) - 1;
+
         if (committedOffset < 0) {
             return null;
         }
-        SimpleConsumer consumer = createConsumer(topicPartition);
+        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(
+                mConfig.createPropsConfig(),
+                new StringDeserializer(),
+                new ByteArrayDeserializer());
+
+        ArrayList<TopicPartition> topicPartitionsList = new ArrayList<TopicPartition>();
+        topicPartitionsList.add(topicPartition);
+        consumer.assign(topicPartitionsList);
         return getMessage(topicPartition, committedOffset, consumer);
     }
+
+    public void setCommitedOffset(String group, String topic, int partition, int offset) throws Exception {
+
+        Properties props = mConfig.createPropsConfig();
+        props.put("group.id", group);
+        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(
+                props,
+                new StringDeserializer(),
+                new ByteArrayDeserializer());
+
+        TopicPartition topicPartition = new TopicPartition(topic, partition);
+        ArrayList<TopicPartition> topicPartitionsList = new ArrayList<TopicPartition>();
+        topicPartitionsList.add(topicPartition);
+        consumer.assign(topicPartitionsList);
+
+        consumer.seek(topicPartition,offset);
+
+        consumer.close();
+    }
 }
diff --git a/src/main/java/com/pinterest/secor/common/OffsetTracker.java b/src/main/java/com/pinterest/secor/common/OffsetTracker.java
index 7aa976cf84026dbd38ca6070e2f00a6f8d245cf0..b28ce343df62fed3aecf3446a4b7a85041c6778e 100644
--- a/src/main/java/com/pinterest/secor/common/OffsetTracker.java
+++ b/src/main/java/com/pinterest/secor/common/OffsetTracker.java
@@ -19,6 +19,7 @@ package com.pinterest.secor.common;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.kafka.common.TopicPartition;
 import java.util.HashMap;
 
 /**
@@ -52,12 +53,11 @@ public class OffsetTracker {
         mLastSeenOffset.put(topicPartition, offset);
         if (lastSeenOffset + 1 != offset) {
             if (lastSeenOffset >= 0) {
-                LOG.warn("offset for topic " + topicPartition.getTopic() + " partition " +
-                        topicPartition.getPartition() + " changed from " + lastSeenOffset + " to " +
-                        offset);
+                LOG.warn("offset for topic {} partition {} changed from {} to {}"
+                        ,topicPartition.topic(), topicPartition.partition(), lastSeenOffset, offset);
             } else {
-                LOG.info("starting to consume topic " + topicPartition.getTopic() + " partition " +
-                        topicPartition.getPartition() + " from offset " + offset);
+                LOG.info("starting to consume topic {} partition {} from offset {}",
+                        topicPartition.topic(), topicPartition.partition(), offset);
             }
         }
         if (mFirstSeendOffset.get(topicPartition) == null) {
diff --git a/src/main/java/com/pinterest/secor/common/OstrichAdminService.java b/src/main/java/com/pinterest/secor/common/OstrichAdminService.java
deleted file mode 100644
index cc2adebbbe3cb4f67d1928808fe636124b1a5497..0000000000000000000000000000000000000000
--- a/src/main/java/com/pinterest/secor/common/OstrichAdminService.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.pinterest.secor.common;
-
-import java.util.Properties;
-import java.util.concurrent.TimeUnit;
-
-import com.pinterest.secor.util.StatsUtil;
-import com.twitter.ostrich.admin.*;
-import com.twitter.util.Duration;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import scala.Option;
-import scala.collection.Map$;
-import scala.collection.immutable.List;
-import scala.collection.immutable.List$;
-import scala.util.matching.Regex;
-
-/**
- * OstrichAdminService initializes export of metrics to Ostrich.
- *
- * @author Pawel Garbacki (pawel@pinterest.com)
- */
-public class OstrichAdminService {
-    private static final Logger LOG = LoggerFactory.getLogger(OstrichAdminService.class);
-    private final int mPort;
-
-    public OstrichAdminService(int port) {
-        this.mPort = port;
-    }
-
-    public void start() {
-        Duration[] defaultLatchIntervals = {Duration.apply(1, TimeUnit.MINUTES)};
-        @SuppressWarnings("deprecation")
-        AdminServiceFactory adminServiceFactory = new AdminServiceFactory(
-            this.mPort,
-            20,
-            List$.MODULE$.<StatsFactory>empty(),
-            Option.<String>empty(),
-            List$.MODULE$.<Regex>empty(),
-            Map$.MODULE$.<String, CustomHttpHandler>empty(),
-            List.<Duration>fromArray(defaultLatchIntervals)
-        );
-        RuntimeEnvironment runtimeEnvironment = new RuntimeEnvironment(this);
-        adminServiceFactory.apply(runtimeEnvironment);
-        try {
-            Properties properties = new Properties();
-            properties.load(this.getClass().getResource("build.properties").openStream());
-            String buildRevision = properties.getProperty("build_revision", "unknown");
-            LOG.info("build.properties build_revision: {}",
-                     properties.getProperty("build_revision", "unknown"));
-            StatsUtil.setLabel("secor.build_revision", buildRevision);
-        } catch (Throwable t) {
-            LOG.error("Failed to load properties from build.properties", t);
-        }
-    }
-}
diff --git a/src/main/java/com/pinterest/secor/common/SecorConfig.java b/src/main/java/com/pinterest/secor/common/SecorConfig.java
index 2cb7c3b9de489332b6fc493f86c5007abd060b8c..77ca61f406b018bc226e450c3301693bf6207ed0 100644
--- a/src/main/java/com/pinterest/secor/common/SecorConfig.java
+++ b/src/main/java/com/pinterest/secor/common/SecorConfig.java
@@ -16,12 +16,18 @@
  */
 package com.pinterest.secor.common;
 
+import com.pinterest.secor.util.IdUtil;
+
 import org.apache.commons.configuration.ConfigurationException;
 import org.apache.commons.configuration.PropertiesConfiguration;
 import org.apache.commons.lang.StringUtils;
 
 import java.util.Map;
 import java.util.Properties;
+import java.net.UnknownHostException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 
 /**
  * One-stop shop for Secor configuration options.
@@ -29,10 +35,12 @@ import java.util.Properties;
  * @author Pawel Garbacki (pawel@pinterest.com)
  */
 public class SecorConfig {
+    private final Logger logger = LoggerFactory.getLogger(SecorConfig.class);
     private final PropertiesConfiguration mProperties;
-
+    
+    
     private static final ThreadLocal<SecorConfig> mSecorConfig = new ThreadLocal<SecorConfig>() {
-
+ 
         @Override
         protected SecorConfig initialValue() {
             // Load the default configuration file first
@@ -67,14 +75,79 @@ public class SecorConfig {
         mProperties = properties;
     }
 
+    public String getKafkaSeedBrokerHostsAndPorts() {
+        String seedBrokerHosts = getString("kafka.seed.broker.host");
+        if (seedBrokerHosts.indexOf(',') != -1) {
+            String[] hosts = seedBrokerHosts.split(",");
+            seedBrokerHosts = "";
+            for (String host: hosts) {
+                if (host.indexOf(':') == -1) {
+                    seedBrokerHosts += host + ":" + getKafkaSeedBrokerPort() + ",";
+                } else {
+                    seedBrokerHosts += host + ",";
+                }
+            }
+            seedBrokerHosts = seedBrokerHosts.substring(0, seedBrokerHosts.length()-1);
+        } else {
+            if (seedBrokerHosts.indexOf(':') == -1) {
+                seedBrokerHosts = seedBrokerHosts + ":" + getKafkaSeedBrokerPort();
+            } 
+        }
+        return seedBrokerHosts;
+    }
+    
     public String getKafkaSeedBrokerHost() {
-        return getString("kafka.seed.broker.host");
+        String host = getString("kafka.seed.broker.host");
+        if (host.indexOf(',') != -1) {
+            host = host.split(",")[0];
+        }        
+        if (host.indexOf(':') != -1) {
+            host = host.split(":")[0];
+        }
+        return host;
     }
 
     public int getKafkaSeedBrokerPort() {
         return getInt("kafka.seed.broker.port");
     }
 
+    public String getKafkaClientId() {
+        return getString("kafka.client.id");
+    }
+    
+    public String getKafkaSecurityProtocol() {
+        return getString("kafka.security.protocol");
+    }
+    
+    public String getKafkaSSLProtocol() {
+        return getString("kafka.ssl.protocol");
+    }
+    
+    public String getKafkaEnabledProtocols() {
+        return getString("kafka.ssl.enabled.protocols");
+    }
+
+
+    private String getKafkaSASLMechanism() {
+        return getString("kafka.sasl.mechanism");
+    }
+    
+    public String getKafkaSSLLocation() {
+        return getString("kafka.ssl.truststore.location");
+    }
+    
+    public String getKafkaSSLPassword() {
+        return getString("kafka.ssl.truststore.password");
+    }
+    
+    public String getKafkaSSLType() {
+        return getString("kafka.ssl.truststore.type");
+    }
+    
+    public String getKafkaSSLIdentAlgorithm() {
+        return getString("kafka.ssl.endpoint.identification.algorithm");
+    }
+    
     public String getKafkaZookeeperPath() {
         return getString("kafka.zookeeper.path");
     }
@@ -135,6 +208,22 @@ public class SecorConfig {
         return getInt("secor.messages.per.second");
     }
 
+    public boolean getSeperateContainersForTopics() {
+    	return getString("secor.swift.containers.for.each.topic").toLowerCase().equals("true");
+    }
+    
+    public String getSwiftContainer() {
+        return getString("secor.swift.container");
+    }
+
+    public String getSwiftPath() {
+        return getString("secor.swift.path");
+    }
+    
+    public String getInfoContainer() {
+    	return getString("secor.info.container");
+    }
+    
     public String getS3Bucket() {
         return getString("secor.s3.bucket");
     }
@@ -155,12 +244,17 @@ public class SecorConfig {
         return getString("secor.kafka.group");
     }
 
-    public int getZookeeperSessionTimeoutMs() {
-        return getInt("zookeeper.session.timeout.ms");
+    public long getZookeeperConnectionTimeoutMs() {
+        return getLong("zookeeper.connection.timeout.ms");
     }
 
-    public int getZookeeperSyncTimeMs() {
-        return getInt("zookeeper.sync.time.ms");
+    public String getZookeeperChroot() {
+        String[] zookeeperQuorumArray = getZookeeperQuorum().split("/");
+        String chroot = "";
+        if (zookeeperQuorumArray.length == 2) {
+            chroot = "/" + zookeeperQuorumArray[1];
+        }
+        return chroot;
     }
 
     public String getMessageParserClass() {
@@ -179,6 +273,10 @@ public class SecorConfig {
         return getInt("ostrich.port");
     }
 
+    public String getCloudService() {
+        return getString("cloud.service");
+    }
+    
     public String getAwsAccessKey() {
         return getString("aws.access.key");
     }
@@ -186,7 +284,47 @@ public class SecorConfig {
     public String getAwsSecretKey() {
         return getString("aws.secret.key");
     }
+    
+    public String getSwiftTenant() {
+        return getString("swift.tenant");
+    }
+    
+    public String getSwiftUsername() {
+        return getString("swift.username");
+    }
+    
+    public String getSwiftPassword() {
+        return getString("swift.password");
+    }    
+    
+    public String getSwiftAuthUrl() {
+        return getString("swift.auth.url");
+    }
+    
+    public String getSwiftPublic() {
+    	return getString("swift.public");
+    }
 
+    public String getSwiftEndpointPrefix() {
+        return getString("swift.endpoint.prefix");
+    }
+    
+    public String getSwiftRegion() {
+        return getString("swift.region");
+    }
+    
+    public String getSwiftPort() {
+    	return getString("swift.port");
+    }
+    
+    public String getSwiftAuthMethod() {
+    	return getString("swift.auth.method");
+    }
+    
+    public String getSwiftApiKey() {
+    	return getString("swift.api.key");
+    }
+    
     public String getQuboleApiToken() {
         return getString("qubole.api.token");
     }
@@ -207,6 +345,10 @@ public class SecorConfig {
         return getString("message.timestamp.name");
     }
 
+    public String getMessageTimestampNameSeparator() {
+        return getString("message.timestamp.name.separator");
+    }
+
     public String getMessageTimestampInputPattern() {
         return getString("message.timestamp.input.pattern");
     }
@@ -231,6 +373,14 @@ public class SecorConfig {
     	return getString("secor.kafka.perf_topic_prefix");
     }
 
+    public int getParquetBlockSize() {
+        return getInt("secor.parquet.block_size");
+    }
+
+    public int getParquetPageSize() {
+        return getInt("secor.parquet.page_size");
+    }
+
     private void checkProperty(String name) {
         if (!mProperties.containsKey(name)) {
             throw new RuntimeException("Failed to find required configuration option '" +
@@ -238,6 +388,73 @@ public class SecorConfig {
         }
     }
 
+    public String getHostFromUrl(String url) throws ConfigurationException {
+        int colonIdx = url.indexOf(':');
+        if (colonIdx == -1) {
+            throw new ConfigurationException("No ':' in url: " + url);
+        }
+        return url.substring(0,colonIdx);
+    }
+
+    public int getPortFromUrl(String url) throws ConfigurationException {
+        int colonIdx = url.indexOf(':');
+        if (colonIdx == -1) {
+            throw new ConfigurationException("No ':' in url: " + url);
+        }
+        return Integer.parseInt(url.substring(colonIdx + 1));
+    }
+
+    public Properties createPropsConfig() throws UnknownHostException {
+        Properties props = new Properties();
+        props.put("group.id", getKafkaGroup());
+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
+                getKafkaSeedBrokerHostsAndPorts());
+
+        props.put("enable.auto.commit", "false");
+        // This option is required to make sure that messages are not lost for new topics and
+        // topics whose number of partitions has changed.
+        props.put("auto.offset.reset", "earliest");
+
+        // This option make all the consumers get topic assignments 
+        props.put("partition.assignment.strategy", "org.apache.kafka.clients.consumer.RangeAssignor");
+        props.put("client.id", IdUtil.getConsumerId());
+        if (getRebalanceMaxRetries() != null &&
+            !getRebalanceMaxRetries().isEmpty()) {
+            props.put("rebalance.max.retries", getRebalanceMaxRetries());
+        }
+        if (getRebalanceBackoffMs() != null &&
+            !getRebalanceBackoffMs().isEmpty()) {
+            props.put("rebalance.backoff.ms", getRebalanceBackoffMs());
+        }
+        if (getSocketReceieveBufferBytes() != null &&
+            !getSocketReceieveBufferBytes().isEmpty()) {
+            props.put("socket.receive.buffer.bytes", getSocketReceieveBufferBytes());
+        }
+        if (getFetchMessageMaxBytes() != null && !getFetchMessageMaxBytes().isEmpty()) {
+            props.put("fetch.message.max.bytes", getFetchMessageMaxBytes());
+        }
+        if (getFetchMinBytes() != null && !getFetchMinBytes().isEmpty()) {
+            props.put("fetch.min.bytes", getFetchMinBytes());
+        }
+        if (getFetchWaitMaxMs() != null && !getFetchWaitMaxMs().isEmpty()) {
+            props.put("fetch.wait.max.ms", getFetchWaitMaxMs());
+        }
+        
+        
+        if (getKafkaSecurityProtocol() != null && !getKafkaSecurityProtocol().isEmpty()) {
+            props.put("security.protocol", getKafkaSecurityProtocol());
+            props.put("sasl.mechanism", getKafkaSASLMechanism());
+            props.put("ssl.protocol", getKafkaSSLProtocol());
+            props.put("ssl.enabled.protocols", getKafkaEnabledProtocols());
+            props.put("ssl.truststore.location", getKafkaSSLLocation());
+            props.put("ssl.truststore.password", getKafkaSSLPassword());
+            props.put("ssl.truststore.type", getKafkaSSLType());
+            props.put("ssl.endpoint.identification.algorithm", getKafkaSSLIdentAlgorithm());
+        }
+        
+        return props;
+    }
+
     private String getString(String name) {
         checkProperty(name);
         return mProperties.getString(name);
diff --git a/src/main/java/com/pinterest/secor/common/SecorReports.java b/src/main/java/com/pinterest/secor/common/SecorReports.java
new file mode 100644
index 0000000000000000000000000000000000000000..77837c0969283663d1802e68f479f9ec6c640628
--- /dev/null
+++ b/src/main/java/com/pinterest/secor/common/SecorReports.java
@@ -0,0 +1,32 @@
+package com.pinterest.secor.common;
+
+/**
+ * Created by guy on 8/22/16.
+ */
+public class SecorReports {
+    private long timestamp;
+    private String status;
+    private boolean hangs;
+
+    public SecorReports(long timestamp, String status, boolean hangs) {
+        this.timestamp = timestamp;
+        this.status = status;
+        this.hangs = hangs;
+    }
+
+    public long getTimestamp() {
+        return timestamp;
+    }
+
+    public String getStatus() {
+        return status;
+    }
+
+    public boolean isHangs() {
+        return hangs;
+    }
+
+    public void setHangs(boolean hangs) {
+        this.hangs = hangs;
+    }
+}
diff --git a/src/main/java/com/pinterest/secor/common/TopicPartition.java b/src/main/java/com/pinterest/secor/common/TopicPartition.java
deleted file mode 100644
index 03e660326863afbfd66cc3277f2fdd4da82bebcb..0000000000000000000000000000000000000000
--- a/src/main/java/com/pinterest/secor/common/TopicPartition.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.pinterest.secor.common;
-
-/**
- * Topic partition describes a kafka message topic-partition pair.
- *
- * @author Pawel Garbacki (pawel@pinterest.com)
- */
-public class TopicPartition {
-    private String mTopic;
-    private int mPartition;
-
-    public TopicPartition(String topic, int partition) {
-        mTopic = topic;
-        mPartition = partition;
-    }
-
-    public String getTopic() {
-        return mTopic;
-    }
-
-    public int getPartition() {
-        return mPartition;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        TopicPartition that = (TopicPartition) o;
-
-        if (mPartition != that.mPartition) return false;
-        if (mTopic != null ? !mTopic.equals(that.mTopic) : that.mTopic != null) return false;
-
-        return true;
-    }
-
-    @Override
-    public int hashCode() {
-        int result = mTopic != null ? mTopic.hashCode() : 0;
-        result = 31 * result + mPartition;
-        return result;
-    }
-
-    @Override
-    public String toString() {
-        return "TopicPartition{" +
-                "mTopic='" + mTopic + '\'' +
-                ", mPartition=" + mPartition +
-                '}';
-    }
-}
diff --git a/src/main/java/com/pinterest/secor/common/ZookeeperConnector.java b/src/main/java/com/pinterest/secor/common/ZookeeperConnector.java
index 62287c94c35691b65703d113f8bb264ec7034b38..29cd153cbbbeabcd5851826d44f073286ec2070b 100644
--- a/src/main/java/com/pinterest/secor/common/ZookeeperConnector.java
+++ b/src/main/java/com/pinterest/secor/common/ZookeeperConnector.java
@@ -24,6 +24,7 @@ import com.twitter.common.zookeeper.DistributedLock;
 import com.twitter.common.zookeeper.DistributedLockImpl;
 import com.twitter.common.zookeeper.ZooKeeperClient;
 import org.apache.commons.lang.StringUtils;
+import org.apache.kafka.common.TopicPartition;
 import org.apache.zookeeper.CreateMode;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.ZooDefs;
@@ -35,6 +36,7 @@ import java.net.InetSocketAddress;
 import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
+import java.util.concurrent.TimeoutException;
 
 /**
  * ZookeeperConnector implements interactions with Zookeeper.
@@ -52,14 +54,15 @@ public class ZookeeperConnector {
     protected ZookeeperConnector() {
     }
 
-    public ZookeeperConnector(SecorConfig config) {
+    public ZookeeperConnector(SecorConfig config) throws InterruptedException, ZooKeeperClient.ZooKeeperConnectionException, TimeoutException {
         mConfig = config;
         mZookeeperClient = new ZooKeeperClient(Amount.of(1, Time.DAYS), getZookeeperAddresses());
+        mZookeeperClient.get(Amount.of(mConfig.getZookeeperConnectionTimeoutMs(),Time.MILLISECONDS)); // connects to zk to find out any issues
         mLocks = new HashMap<String, DistributedLock>();
     }
 
     private Iterable<InetSocketAddress> getZookeeperAddresses() {
-        String zookeeperQuorum = mConfig.getZookeeperQuorum();
+        String zookeeperQuorum = mConfig.getZookeeperQuorum().split("/")[0]; //cleans prefix if exists
         String[] hostports = zookeeperQuorum.split(",");
         LinkedList<InetSocketAddress> result = new LinkedList<InetSocketAddress>();
         for (String hostport : hostports) {
@@ -73,6 +76,14 @@ public class ZookeeperConnector {
     }
 
     public void lock(String lockPath) {
+        lockPath = mConfig.getZookeeperChroot() + lockPath;
+        try {
+            createMissingParents(lockPath);
+        } catch (Exception e) {
+            LOG.error("Can't create zookeeper path missing parents", e);
+            e.printStackTrace();
+            System.exit(1);
+        }
         assert mLocks.get(lockPath) == null: "mLocks.get(" + lockPath + ") == null";
         DistributedLock distributedLock = new DistributedLockImpl(mZookeeperClient, lockPath);
         mLocks.put(lockPath, distributedLock);
@@ -80,6 +91,7 @@ public class ZookeeperConnector {
     }
 
     public void unlock(String lockPath) {
+        lockPath = mConfig.getZookeeperChroot() + lockPath;
         DistributedLock distributedLock = mLocks.get(lockPath);
         assert distributedLock != null: "mLocks.get(" + lockPath + ") != null";
         distributedLock.unlock();
@@ -105,8 +117,8 @@ public class ZookeeperConnector {
     }
 
     private String getCommittedOffsetPartitionPath(TopicPartition topicPartition) {
-        return getCommittedOffsetTopicPath(topicPartition.getTopic()) + "/" +
-            topicPartition.getPartition();
+        return getCommittedOffsetTopicPath(topicPartition.topic()) + "/" +
+            topicPartition.partition();
     }
 
     public long getCommittedOffsetCount(TopicPartition topicPartition) throws Exception {
@@ -116,7 +128,7 @@ public class ZookeeperConnector {
             byte[] data = zookeeper.getData(offsetPath, false, null);
             return Long.parseLong(new String(data));
         } catch (KeeperException.NoNodeException exception) {
-            LOG.warn("path " + offsetPath + " does not exist in zookeeper");
+            LOG.warn("path {} does not exist in zookeeper", offsetPath);
             return -1;
         }
     }
@@ -156,47 +168,12 @@ public class ZookeeperConnector {
             prefix += "/" + elements[i];
             try {
                 zookeeper.create(prefix, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
-                LOG.info("created path " + prefix);
+                LOG.info("created path {}", prefix);
             } catch (KeeperException.NodeExistsException exception) {
             }
         }
     }
 
-    public void setCommittedOffsetCount(TopicPartition topicPartition, long count)
-            throws Exception {
-        ZooKeeper zookeeper = mZookeeperClient.get();
-        String offsetPath = getCommittedOffsetPartitionPath(topicPartition);
-        LOG.info("creating missing parents for zookeeper path " + offsetPath);
-        createMissingParents(offsetPath);
-        byte[] data = Long.toString(count).getBytes();
-        try {
-            LOG.info("setting zookeeper path " + offsetPath + " value " + count);
-            // -1 matches any version
-            zookeeper.setData(offsetPath, data, -1);
-        } catch (KeeperException.NoNodeException exception) {
-            zookeeper.create(offsetPath, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
-        }
-    }
-
-    public void deleteCommittedOffsetTopicCount(String topic) throws Exception {
-        ZooKeeper zookeeper = mZookeeperClient.get();
-        List<Integer> partitions = getCommittedOffsetPartitions(topic);
-        for (Integer partition : partitions) {
-            TopicPartition topicPartition = new TopicPartition(topic, partition);
-            String offsetPath = getCommittedOffsetPartitionPath(topicPartition);
-            LOG.info("deleting path " + offsetPath);
-            zookeeper.delete(offsetPath, -1);
-        }
-    }
-
-    public void deleteCommittedOffsetPartitionCount(TopicPartition topicPartition)
-            throws Exception {
-        String offsetPath = getCommittedOffsetPartitionPath(topicPartition);
-        ZooKeeper zookeeper = mZookeeperClient.get();
-        LOG.info("deleting path " + offsetPath);
-        zookeeper.delete(offsetPath, -1);
-    }
-
     protected void setConfig(SecorConfig config) {
         this.mConfig = config;
     }
diff --git a/src/main/java/com/pinterest/secor/consumer/Consumer.java b/src/main/java/com/pinterest/secor/consumer/Consumer.java
index 20c830b56352bcc59a9baf9dc7f8b17512bc7c70..448583ae8054cf0ab703a04805fbaf1d158db41c 100644
--- a/src/main/java/com/pinterest/secor/consumer/Consumer.java
+++ b/src/main/java/com/pinterest/secor/consumer/Consumer.java
@@ -19,6 +19,7 @@ package com.pinterest.secor.consumer;
 import com.pinterest.secor.common.FileRegistry;
 import com.pinterest.secor.common.OffsetTracker;
 import com.pinterest.secor.common.SecorConfig;
+import com.pinterest.secor.common.SecorReports;
 import com.pinterest.secor.message.Message;
 import com.pinterest.secor.message.ParsedMessage;
 import com.pinterest.secor.parser.MessageParser;
@@ -27,13 +28,21 @@ import com.pinterest.secor.reader.MessageReader;
 import com.pinterest.secor.util.ReflectionUtil;
 import com.pinterest.secor.writer.MessageWriter;
 
-import kafka.consumer.ConsumerTimeoutException;
-
+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
 import java.lang.Thread;
+import java.lang.management.ManagementFactory;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.regex.Pattern;
 
 /**
  * Consumer is a top-level component coordinating reading, writing, and uploading Kafka log
@@ -45,6 +54,16 @@ import java.lang.Thread;
  *
  * @author Pawel Garbacki (pawel@pinterest.com)
  */
+//not needed right now due to secor design
+class SaveOffsetsOnRebalance implements ConsumerRebalanceListener {
+  public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
+      return;
+  }
+  public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
+     return;
+  }
+}
+
 public class Consumer extends Thread {
     private static final Logger LOG = LoggerFactory.getLogger(Consumer.class);
 
@@ -55,21 +74,55 @@ public class Consumer extends Thread {
     private MessageParser mMessageParser;
     private OffsetTracker mOffsetTracker;
     private Uploader mUploader;
+    private Set<TopicPartition> assignedTP;
+
+    public Set<TopicPartition> getAssignedTP() {
+        return assignedTP != null ? assignedTP : new HashSet<TopicPartition>();
+    }
+
+
+    private KafkaConsumer<String, byte[]> mKafkaConsumer;
     // TODO(pawel): we should keep a count per topic partition.
     private double mUnparsableMessages;
+    public SecorReports mReport;
+
+    // used for testing
+    public SecorReports getmReport() {
+        return mReport;
+    }
+
 
     public Consumer(SecorConfig config) {
         mConfig = config;
+        mReport = new SecorReports(System.currentTimeMillis(), "Constructing Consumer", false);
     }
 
     private void init() throws Exception {
         mOffsetTracker = new OffsetTracker();
-        mMessageReader = new MessageReader(mConfig, mOffsetTracker);
         FileRegistry fileRegistry = new FileRegistry(mConfig);
         mMessageWriter = new MessageWriter(mConfig, mOffsetTracker, fileRegistry);
         mMessageParser = ReflectionUtil.createMessageParser(mConfig.getMessageParserClass(), mConfig);
-        mUploader = new Uploader(mConfig, mOffsetTracker, fileRegistry);
         mUnparsableMessages = 0.;
+        
+        mKafkaConsumer = new KafkaConsumer<String, byte[]>(
+                mConfig.createPropsConfig(),
+                new StringDeserializer(),
+                new ByteArrayDeserializer());
+
+        // exclude the topic that track the consumers offsets
+        String offsetsTopicsRegex = "(?=(?!__consumer_offsets))";
+        Pattern pattern = Pattern.compile(offsetsTopicsRegex + mConfig.getKafkaTopicFilter());
+        ConsumerRebalanceListener listener = new SaveOffsetsOnRebalance();
+        String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
+        LOG.info("kafkaConsumer (pid: " + pid + ") consumer before subscribe");
+        mKafkaConsumer.subscribe(pattern, listener); // TODO: need to add  ConsumerRebalanceListener here
+        LOG.info("kafkaConsumer (pid: " + pid + ") consumer subscribed");
+
+        mReport = new SecorReports(System.currentTimeMillis(), "Creating uploader, connecting zookeeper", true);
+        mUploader = new Uploader(mConfig, mOffsetTracker, fileRegistry, mKafkaConsumer);
+        mReport.setHangs(false);
+        mMessageReader = new MessageReader(mConfig, mOffsetTracker, mKafkaConsumer);
+        assignedTP = mKafkaConsumer.assignment();
     }
 
     @Override
@@ -87,19 +140,16 @@ public class Consumer extends Thread {
         long nMessages = 0;
         long lastChecked = System.currentTimeMillis();
         while (true) {
-            boolean hasMoreMessages = consumeNextMessage();
-            if (!hasMoreMessages) {
-                break;
-            }
+            consumeNextMessage();
 
             long now = System.currentTimeMillis();
             if (nMessages++ % checkMessagesPerSecond == 0 ||
                     (now - lastChecked) > checkEveryNSeconds * 1000) {
+                assignedTP = mKafkaConsumer.assignment();
                 lastChecked = now;
                 checkUploadPolicy();
             }
         }
-        checkUploadPolicy();
     }
 
     private void checkUploadPolicy() {
@@ -112,18 +162,16 @@ public class Consumer extends Thread {
 
     // @return whether there are more messages left to consume
     private boolean consumeNextMessage() {
-        Message rawMessage = null;
-        try {
-            boolean hasNext = mMessageReader.hasNext();
-            if (!hasNext) {
-                return false;
-            }
-            rawMessage = mMessageReader.read();
-        } catch (ConsumerTimeoutException e) {
-            // We wait for a new message with a timeout to periodically apply the upload policy
-            // even if no messages are delivered.
-            LOG.trace("Consumer timed out", e);
+        mReport = new SecorReports(System.currentTimeMillis(), "Consuming Next Message", true);
+        boolean hasNext = mMessageReader.hasNext();
+        mReport.setHangs(false);
+        if (!hasNext) {
+            return false;
         }
+        mReport = new SecorReports(System.currentTimeMillis(), "Reading Message", true);
+        Message rawMessage = mMessageReader.read();
+        mReport.setHangs(false);
+
         if (rawMessage != null) {
             // Before parsing, update the offset and remove any redundant data
             try {
@@ -136,18 +184,18 @@ public class Consumer extends Thread {
                 parsedMessage = mMessageParser.parse(rawMessage);
                 final double DECAY = 0.999;
                 mUnparsableMessages *= DECAY;
-            } catch (Exception e) {
+            } catch (Throwable e) {
                 mUnparsableMessages++;
                 final double MAX_UNPARSABLE_MESSAGES = 1000.;
                 if (mUnparsableMessages > MAX_UNPARSABLE_MESSAGES) {
                     throw new RuntimeException("Failed to parse message " + rawMessage, e);
                 }
-                LOG.warn("Failed to parse message " + rawMessage, e);
+                LOG.warn("Failed to parse message {}", rawMessage, e);
             }
 
             if (parsedMessage != null) {
                 try {
-                    mMessageWriter.write(parsedMessage);
+                	mMessageWriter.write(parsedMessage);
                 } catch (Exception e) {
                     throw new RuntimeException("Failed to write message " + parsedMessage, e);
                 }
diff --git a/src/main/java/com/pinterest/secor/io/impl/DelimitedTextFileReaderWriterFactory.java b/src/main/java/com/pinterest/secor/io/impl/DelimitedTextFileReaderWriterFactory.java
index f0d1738c3cb0e49cac3fc96fa95527347f2bcaa2..644e4b185aabe9436396f9fc964a1eae694c9d09 100644
--- a/src/main/java/com/pinterest/secor/io/impl/DelimitedTextFileReaderWriterFactory.java
+++ b/src/main/java/com/pinterest/secor/io/impl/DelimitedTextFileReaderWriterFactory.java
@@ -29,6 +29,9 @@ import com.pinterest.secor.io.FileWriter;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.Compressor;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.Decompressor;
 
 import com.google.common.io.CountingOutputStream;
 import com.pinterest.secor.common.LogFilePath;
@@ -57,6 +60,7 @@ public class DelimitedTextFileReaderWriterFactory implements FileReaderWriterFac
     protected class DelimitedTextFileReader implements FileReader {
         private final BufferedInputStream mReader;
         private long mOffset;
+        private Decompressor mDecompressor = null;
 
         public DelimitedTextFileReader(LogFilePath path, CompressionCodec codec) throws IOException {
             Path fsPath = new Path(path.getLogFilePath());
@@ -64,7 +68,8 @@ public class DelimitedTextFileReaderWriterFactory implements FileReaderWriterFac
             InputStream inputStream = fs.open(fsPath);
             this.mReader = (codec == null) ? new BufferedInputStream(inputStream)
                     : new BufferedInputStream(
-                    codec.createInputStream(inputStream));
+                    codec.createInputStream(inputStream,
+                            mDecompressor = CodecPool.getDecompressor(codec)));
             this.mOffset = path.getOffset();
         }
 
@@ -89,12 +94,17 @@ public class DelimitedTextFileReaderWriterFactory implements FileReaderWriterFac
         @Override
         public void close() throws IOException {
             this.mReader.close();
+            if (mDecompressor != null) {
+                CodecPool.returnDecompressor(mDecompressor);
+                mDecompressor = null;
+            }
         }
     }
 
     protected class DelimitedTextFileWriter implements FileWriter {
         private final CountingOutputStream mCountingStream;
         private final BufferedOutputStream mWriter;
+        private Compressor mCompressor = null;
 
         public DelimitedTextFileWriter(LogFilePath path, CompressionCodec codec) throws IOException {
             Path fsPath = new Path(path.getLogFilePath());
@@ -102,7 +112,8 @@ public class DelimitedTextFileReaderWriterFactory implements FileReaderWriterFac
             this.mCountingStream = new CountingOutputStream(fs.create(fsPath));
             this.mWriter = (codec == null) ? new BufferedOutputStream(
                     this.mCountingStream) : new BufferedOutputStream(
-                    codec.createOutputStream(this.mCountingStream));
+                    codec.createOutputStream(this.mCountingStream,
+                            mCompressor = CodecPool.getCompressor(codec)));
         }
 
         @Override
@@ -120,6 +131,10 @@ public class DelimitedTextFileReaderWriterFactory implements FileReaderWriterFac
         @Override
         public void close() throws IOException {
             this.mWriter.close();
+            if (mCompressor !=  null) {
+                CodecPool.returnCompressor(mCompressor);
+                mCompressor = null;
+            }
         }
     }
 }
\ No newline at end of file
diff --git a/src/main/java/com/pinterest/secor/io/impl/ParquetFileReaderWriterFactory.java b/src/main/java/com/pinterest/secor/io/impl/ParquetFileReaderWriterFactory.java
new file mode 100644
index 0000000000000000000000000000000000000000..c0a211e54906028106309d1c7d02c66dd82a3477
--- /dev/null
+++ b/src/main/java/com/pinterest/secor/io/impl/ParquetFileReaderWriterFactory.java
@@ -0,0 +1,251 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.pinterest.secor.io.impl;
+
+import java.io.IOException;
+import org.apache.commons.configuration.ConfigurationException;
+import java.io.InputStream;
+import java.util.Iterator;
+import java.util.List;
+import java.util.ArrayList;
+
+import com.pinterest.secor.io.FileReader;
+import com.pinterest.secor.io.FileReaderWriterFactory;
+import com.pinterest.secor.io.FileWriter;
+import com.pinterest.secor.common.LogFilePath;
+import com.pinterest.secor.io.KeyValue;
+import com.pinterest.secor.util.FileUtil;
+import com.pinterest.secor.common.SecorConfig;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.compress.CompressionCodec;
+
+import parquet.avro.AvroParquetWriter;
+import parquet.avro.AvroParquetReader;
+import parquet.hadoop.metadata.CompressionCodecName;
+
+import org.apache.avro.generic.GenericData;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.Schema;
+import org.json.JSONObject;
+import org.json.JSONArray;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Parquet Formatted File Reader Writer with Compression
+ *
+ * @author Guy Hadash (guyh@il.ibm.com)
+ */
+public class ParquetFileReaderWriterFactory implements FileReaderWriterFactory {
+	private static final Logger LOG = LoggerFactory.getLogger(ParquetFileReaderWriterFactory.class);
+	
+    @Override
+    public FileReader BuildFileReader(LogFilePath logFilePath, CompressionCodec codec)
+            throws IllegalAccessException, IOException, InstantiationException {
+        return new ParquetTextFileReader(logFilePath, codec);
+    }
+
+    @Override
+    public FileWriter BuildFileWriter(LogFilePath logFilePath, CompressionCodec codec) throws IOException {
+        return new ParquetTextFileWriter(logFilePath, codec);
+    }
+
+    protected class ParquetTextFileReader implements FileReader {
+        private final AvroParquetReader mReader;
+        private long mOffset;
+
+        public ParquetTextFileReader(LogFilePath path, CompressionCodec codec) throws IOException {
+            Path fsPath = new Path(path.getLogFilePath());
+            Configuration config = new Configuration();
+            this.mReader = new AvroParquetReader(config,fsPath);
+            this.mOffset = path.getOffset();
+        }
+
+        @Override
+        public KeyValue next() throws IOException {
+        	GenericRecord messageBuffer = (GenericRecord) mReader.read();
+            if (messageBuffer != null) {
+            	String temp = messageBuffer.toString();
+            	return new KeyValue(this.mOffset++, temp.getBytes());
+            }
+            return null;
+        }
+
+        @Override
+        public void close() throws IOException {
+            this.mReader.close();
+        }
+    }
+
+    protected class ParquetTextFileWriter implements FileWriter {
+        private final AvroParquetWriter mWriter;
+        private long length;
+        private Schema schema;
+        private String originalPath;
+        private SecorConfig secorConf;
+        
+        public ParquetTextFileWriter(LogFilePath path, CompressionCodec codec) throws IOException {
+        	try {
+        		this.secorConf = SecorConfig.load();
+        	} catch (ConfigurationException e) {
+    			throw new IOException(e);
+    		}
+        	String rawSchema = getInfoByName(path.getTopic(),"schema");
+        	this.schema = new Schema.Parser().parse(rawSchema);
+            Path fsPath = new Path(path.getLogFilePath());
+            this.originalPath = path.getLogFilePath();
+            //TODO: fix length (counts how many bytes needs to be written)
+			this.length = 0;
+
+			
+			CompressionCodecName compressionCodecName = CompressionCodecName.UNCOMPRESSED;
+            		// set Parquet file block size and page size values
+            		int blockSize = this.secorConf.getParquetBlockSize(); //256 * 1024 * 1024;
+            		int pageSize = this.secorConf.getParquetPageSize(); //64 * 1024;
+			
+			if (codec != null) {
+				String extension = codec.getDefaultExtension();
+				if (extension.equals(".gz")) {
+					compressionCodecName = CompressionCodecName.GZIP;
+				} else if (extension.equals(".snappy")) {
+					compressionCodecName = CompressionCodecName.SNAPPY;
+				} else if (extension.equals(".lzo")) {
+					compressionCodecName = CompressionCodecName.LZO;
+				}
+			}
+            this.mWriter = new AvroParquetWriter(fsPath, schema, compressionCodecName, blockSize, pageSize);
+        }
+
+        
+        private String getInfoByName(String name, String type) throws IOException {
+        	String container = this.secorConf.getInfoContainer();
+        	String prefix = null;
+    		// the first condition is for compile tests       	
+        	if (secorConf.getCloudService() != null && secorConf.getCloudService().equals("Swift")) { 	
+				prefix = "swift2d://" + container + ".GENERICPROJECT/";
+			} else {
+				prefix = "s3n://" + container + "/";
+			}
+        	String schemaUrl = prefix + name + "." + type;
+        	Path schemaPath = new Path(schemaUrl);
+        	FileSystem fs = FileUtil.getFileSystem(schemaUrl);
+        	InputStream input = null;
+        	try {
+        		input = fs.open(schemaPath);
+                java.util.Scanner s = new java.util.Scanner(input).useDelimiter("\\A");
+                String rawData = s.hasNext() ? s.next() : "";
+                input.close();
+                fs.close();
+                return rawData;
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }	
+        }
+
+        @Override
+        public long getLength() throws IOException {
+        	return this.length;
+        }
+
+
+        @Override
+        public void write(KeyValue keyValue) throws IOException {
+		Object data = toObject(new String(keyValue.getValue()), schema, "", "");
+		if (data != null) { this.mWriter.write(data); }
+	}
+
+    Object toObject(String rawObject, Schema currSchema, String nestedPath, String key) {
+        Object returnObj = null;
+
+        String nestedKey = nestedPath + key;
+        boolean min = false, max = false;
+        switch(currSchema.getType()) {
+            case RECORD:
+                GenericRecord data = new GenericData.Record(currSchema);
+                JSONObject jObject = new JSONObject(rawObject);
+                Iterator<?> keys = jObject.keys();
+                if ( !key.equals("") ) { nestedPath = nestedPath + key + "/"; }
+                while( keys.hasNext() ) {
+                    try {
+                        String recordKey = (String)keys.next();
+                        String value = jObject.get(recordKey).toString();
+                        Schema tmpSchema = currSchema.getField(recordKey).schema();
+                        Object valueObj = toObject(value, tmpSchema, nestedPath, recordKey);
+                        data.put(recordKey, valueObj);
+                    } catch (java.lang.NumberFormatException e) {
+                        LOG.warn("PARQUET: got wrong data - skipped it");
+                        LOG.warn("PARQUET: current KEY: " + key);
+                        LOG.warn("PARQUET: raw data:\n" + rawObject);
+                    }
+                }
+                returnObj = data;
+                break;
+            case UNION:
+               List<Schema> schemaList = this.schema.getField(key).schema().getTypes();
+               for (Schema eachSchema: schemaList) {
+                   if (eachSchema.getType() != Schema.Type.NULL) {
+                       returnObj = toObject(rawObject, eachSchema, nestedPath, key);
+                        break;
+                    }
+                }
+                break;
+            case ARRAY:
+                Schema elementSchema = currSchema.getElementType();
+
+                List<Object> objectsList = new ArrayList<Object>();
+                JSONArray jArray =  new JSONArray(rawObject);
+                for ( int i=0 ; i<jArray.length(); i++) {
+                    objectsList.add(toObject(jArray.get(i).toString(), elementSchema, nestedPath, key));
+                }
+                returnObj = new GenericData.Array(currSchema, objectsList);
+                break;
+            case FLOAT:
+                returnObj = Float.parseFloat(rawObject);
+                break;
+            case INT:
+                returnObj = Integer.parseInt(rawObject);
+                break;
+            case LONG:
+                returnObj = Long.parseLong(rawObject);
+                break;
+            case DOUBLE:
+                returnObj = Double.parseDouble(rawObject);
+                break;
+            case STRING:
+                returnObj = rawObject;
+                break;
+            default:
+                returnObj = rawObject;
+            }
+
+        if (key.equals("")) {
+                int lengthAddition = rawObject.length();
+                this.length += lengthAddition > 10 ? lengthAddition / 10 : lengthAddition / 2 ;
+        }
+        return returnObj;
+    }
+
+        @Override
+        public void close() throws IOException {
+            this.mWriter.close();
+        }
+    }
+}
diff --git a/src/main/java/com/pinterest/secor/main/ConsumerMain.java b/src/main/java/com/pinterest/secor/main/ConsumerMain.java
index ac751edfb87954d955e835d9acd5156d3d59d6c4..c27fe280197436b75b1ce968c59fe4bdc293ebc2 100644
--- a/src/main/java/com/pinterest/secor/main/ConsumerMain.java
+++ b/src/main/java/com/pinterest/secor/main/ConsumerMain.java
@@ -16,9 +16,9 @@
  */
 package com.pinterest.secor.main;
 
-import com.pinterest.secor.common.OstrichAdminService;
 import com.pinterest.secor.common.SecorConfig;
 import com.pinterest.secor.consumer.Consumer;
+import com.pinterest.secor.supervisor.Supervisor;
 import com.pinterest.secor.tools.LogFileDeleter;
 import com.pinterest.secor.util.FileUtil;
 import com.pinterest.secor.util.RateLimitUtil;
@@ -52,8 +52,6 @@ public class ConsumerMain {
         }
         try {
             SecorConfig config = SecorConfig.load();
-            OstrichAdminService ostrichService = new OstrichAdminService(config.getOstrichPort());
-            ostrichService.start();
             FileUtil.configure(config);
 
             LogFileDeleter logFileDeleter = new LogFileDeleter(config);
@@ -72,11 +70,17 @@ public class ConsumerMain {
                 Consumer consumer = new Consumer(config);
                 consumer.setUncaughtExceptionHandler(handler);
                 consumers.add(consumer);
+                LOG.info("Secor consumer initialized");
                 consumer.start();
             }
+            Supervisor supervisor = new Supervisor(consumers, config);
+            supervisor.setUncaughtExceptionHandler(handler);
+            supervisor.run();
             for (Consumer consumer : consumers) {
                 consumer.join();
+                LOG.info("Secor consumer joined");
             }
+            LOG.info("Secor existed");
         } catch (Throwable t) {
             LOG.error("Consumer failed", t);
             System.exit(1);
diff --git a/src/main/java/com/pinterest/secor/main/LogFileVerifierMain.java b/src/main/java/com/pinterest/secor/main/LogFileVerifierMain.java
index ec1d7ec406af0115d2ea649404a8e73d9e3dcad1..5781603030e2c0d084daf38f8bbfd16b2622f22a 100644
--- a/src/main/java/com/pinterest/secor/main/LogFileVerifierMain.java
+++ b/src/main/java/com/pinterest/secor/main/LogFileVerifierMain.java
@@ -80,14 +80,14 @@ public class LogFileVerifierMain {
                 commandLine.getOptionValue("topic"));
             long startOffset = -2;
             long endOffset = Long.MAX_VALUE;
-            if (commandLine.hasOption("start_offset")) {
+        	if (commandLine.hasOption("start_offset")) {
                 startOffset = Long.parseLong(commandLine.getOptionValue("start_offset"));
                 if (commandLine.hasOption("end_offset")) {
                     endOffset = Long.parseLong(commandLine.getOptionValue("end_offset"));
                 }
             }
             int numMessages = -1;
-            if (commandLine.hasOption("messages")) {
+        	if (commandLine.hasOption("messages")) {
                 numMessages = ((Number) commandLine.getParsedOptionValue("messages")).intValue();
             }
             verifier.verifyCounts(startOffset, endOffset, numMessages);
diff --git a/src/main/java/com/pinterest/secor/main/ZookeeperClientMain.java b/src/main/java/com/pinterest/secor/main/ZookeeperClientMain.java
deleted file mode 100644
index 946b0fdd1e4440e10b60e8f760cd9c0c9e9afe17..0000000000000000000000000000000000000000
--- a/src/main/java/com/pinterest/secor/main/ZookeeperClientMain.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.pinterest.secor.main;
-
-import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
-import com.pinterest.secor.common.ZookeeperConnector;
-import org.apache.commons.cli.*;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Zookeeper client main.
- *
- * Run:
- *     $ cd optimus/secor
- *     $ mvn package
- *     $ cd target
- *     $ java -ea -Dlog4j.configuration=log4j.dev.properties -Dconfig=secor.dev.backup.properties \
- *         -cp "secor-0.1-SNAPSHOT.jar:lib/*" com.pinterest.secor.main.ZookeeperClientMain -c \
- *         delete_committed_offsets -t test -p 0
- *
- * @author Pawel Garbacki (pawel@pinterest.com)
- */
-public class ZookeeperClientMain {
-    private static final Logger LOG = LoggerFactory.getLogger(LogFilePrinterMain.class);
-
-    private static CommandLine parseArgs(String[] args) throws ParseException {
-        Options options = new Options();
-        options.addOption(OptionBuilder.withLongOpt("command")
-            .withDescription("command name.  One of \"delete_committed_offsets\"")
-            .hasArg()
-            .withArgName("<command>")
-            .withType(String.class)
-            .create("c"));
-        options.addOption(OptionBuilder.withLongOpt("topic")
-                                       .withDescription("topic whose offset should be read")
-                                       .hasArg()
-                                       .withArgName("<topic>")
-                                       .withType(String.class)
-                                       .create("t"));
-        options.addOption(OptionBuilder.withLongOpt("partition")
-            .withDescription("kafka partition whose offset should be read")
-            .hasArg()
-            .withArgName("<partition>")
-            .withType(Number.class)
-            .create("p"));
-
-        CommandLineParser parser = new GnuParser();
-        return parser.parse(options, args);
-    }
-
-    public static void main(String[] args) {
-        try {
-            CommandLine commandLine = parseArgs(args);
-            String command = commandLine.getOptionValue("command");
-            if (!command.equals("delete_committed_offsets")) {
-                throw new IllegalArgumentException(
-                    "command has to be one of \"delete_committed_offsets\"");
-            }
-            SecorConfig config = SecorConfig.load();
-            ZookeeperConnector zookeeperConnector = new ZookeeperConnector(config);
-            String topic = commandLine.getOptionValue("topic");
-            if (commandLine.hasOption("partition")) {
-                int partition =
-                    ((Number) commandLine.getParsedOptionValue("partition")).intValue();
-                TopicPartition topicPartition = new TopicPartition(topic, partition);
-                zookeeperConnector.deleteCommittedOffsetPartitionCount(topicPartition);
-            } else {
-                zookeeperConnector.deleteCommittedOffsetTopicCount(topic);
-            }
-        } catch (Throwable t) {
-            LOG.error("Zookeeper client failed", t);
-            System.exit(1);
-        }
-    }
-}
diff --git a/src/main/java/com/pinterest/secor/message/Message.java b/src/main/java/com/pinterest/secor/message/Message.java
index c50ba94449c76372e3b202334a1f967f0f2c81a9..00dba35061def1a47c3c0ec4a369ec0ec120edbd 100644
--- a/src/main/java/com/pinterest/secor/message/Message.java
+++ b/src/main/java/com/pinterest/secor/message/Message.java
@@ -49,6 +49,10 @@ public class Message {
         mKafkaPartition = kafkaPartition;
         mOffset = offset;
         mPayload = payload;
+        
+        if (mPayload == null) {
+            mPayload = new byte[0];
+        }
     }
 
     public String getTopic() {
diff --git a/src/main/java/com/pinterest/secor/parser/DateMessageParser.java b/src/main/java/com/pinterest/secor/parser/DateMessageParser.java
index 3370b9352c68af94b8db48c9afb0c8d3c79d6355..1c2b1e3cd5985425601c408357b938856dfa8b80 100644
--- a/src/main/java/com/pinterest/secor/parser/DateMessageParser.java
+++ b/src/main/java/com/pinterest/secor/parser/DateMessageParser.java
@@ -52,7 +52,7 @@ public class DateMessageParser extends MessageParser {
         String result[] = { defaultDate };
 
         if (jsonObject != null) {
-            Object fieldValue = jsonObject.get(mConfig.getMessageTimestampName());
+            Object fieldValue = getJsonFieldValue(jsonObject);
             Object inputPattern = mConfig.getMessageTimestampInputPattern();
             if (fieldValue != null && inputPattern != null) {
                 try {
@@ -62,9 +62,8 @@ public class DateMessageParser extends MessageParser {
                     result[0] = "dt=" + outputFormatter.format(dateFormat);
                     return result;
                 } catch (Exception e) {
-                    LOG.warn("Impossible to convert date = " + fieldValue.toString()
-                            + " for the input pattern = " + inputPattern.toString()
-                            + ". Using date default=" + result[0]);
+                    LOG.warn("Impossible to convert date = {} for the input pattern = {}" + 
+                             ". Using date default={}",fieldValue.toString(), inputPattern.toString(), result[0]);
                 }
             }
         }
diff --git a/src/main/java/com/pinterest/secor/parser/Iso8601MessageParser.java b/src/main/java/com/pinterest/secor/parser/Iso8601MessageParser.java
new file mode 100644
index 0000000000000000000000000000000000000000..e3e2a824aeb7518457f2341cd0867907a8e4c328
--- /dev/null
+++ b/src/main/java/com/pinterest/secor/parser/Iso8601MessageParser.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.pinterest.secor.parser;
+
+import java.text.SimpleDateFormat;
+import java.util.Date;
+
+import net.minidev.json.JSONObject;
+import net.minidev.json.JSONValue;
+import javax.xml.bind.DatatypeConverter;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.pinterest.secor.common.SecorConfig;
+import com.pinterest.secor.message.Message;
+
+/**
+ * Iso8601MessageParser extracts timestamp field (specified by 'message.timestamp.name')
+ *
+ * @author Jurriaan Pruis (email@jurriaanpruis.nl)
+ *
+ */
+public class Iso8601MessageParser extends MessageParser {
+    private static final Logger LOG = LoggerFactory.getLogger(Iso8601MessageParser.class);
+    protected static final String defaultDate = "dt=1970-01-01";
+    protected static final String defaultFormatter = "yyyy-MM-dd";
+    protected static final SimpleDateFormat outputFormatter = new SimpleDateFormat(defaultFormatter);
+
+    public Iso8601MessageParser(SecorConfig config) {
+        super(config);
+    }
+
+    @Override
+    public String[] extractPartitions(Message message) throws Exception {
+        JSONObject jsonObject = (JSONObject) JSONValue.parse(message.getPayload());
+        String result[] = { defaultDate };
+
+        if (jsonObject != null) {
+            Object fieldValue = getJsonFieldValue(jsonObject);
+            if (fieldValue == null) {
+                String msg = String.format("Missing timestamp field value. Using default partition = %s", defaultDate);
+                LOG.warn(msg);
+                LOG.warn("Message: " + new String(message.getPayload()));
+            } else {
+                try {
+                    Date dateFormat = DatatypeConverter.parseDateTime(fieldValue.toString()).getTime();
+                    result[0] = "dt=" + outputFormatter.format(dateFormat);
+                } catch (Exception e) {
+                    String msg = String.format("Impossible to convert date = %s as ISO-8601. Using date default = %s",
+                            fieldValue.toString(), result[0]);
+                    LOG.warn(msg);
+                    LOG.warn("Message: " + new String(message.getPayload()));
+                }
+            }
+        }
+
+        return result;
+    }
+
+}
diff --git a/src/main/java/com/pinterest/secor/parser/JsonMessageParser.java b/src/main/java/com/pinterest/secor/parser/JsonMessageParser.java
index 183bf7aa3c3a6a89fa36e41634e697c062086ed2..687c64c6c7d149695f4862e2e8a7826f426f3a0c 100644
--- a/src/main/java/com/pinterest/secor/parser/JsonMessageParser.java
+++ b/src/main/java/com/pinterest/secor/parser/JsonMessageParser.java
@@ -34,12 +34,11 @@ public class JsonMessageParser extends TimestampedMessageParser {
     public long extractTimestampMillis(final Message message) {
         JSONObject jsonObject = (JSONObject) JSONValue.parse(message.getPayload());
         if (jsonObject != null) {
-            Object fieldValue = jsonObject.get(mConfig.getMessageTimestampName());
+            Object fieldValue = getJsonFieldValue(jsonObject);
             if (fieldValue != null) {
                 return toMillis(Double.valueOf(fieldValue.toString()).longValue());
             }
         }
         return 0;
     }
-
 }
diff --git a/src/main/java/com/pinterest/secor/parser/MessageParser.java b/src/main/java/com/pinterest/secor/parser/MessageParser.java
index b0770760ee31fa3da84c62b40dd0a349120a0d30..b8d19c24eeb24716be3609a5b987fb88c45de6da 100644
--- a/src/main/java/com/pinterest/secor/parser/MessageParser.java
+++ b/src/main/java/com/pinterest/secor/parser/MessageParser.java
@@ -19,6 +19,11 @@ package com.pinterest.secor.parser;
 import com.pinterest.secor.common.SecorConfig;
 import com.pinterest.secor.message.Message;
 import com.pinterest.secor.message.ParsedMessage;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.minidev.json.JSONObject;
+import net.minidev.json.JSONValue;
+import java.util.regex.Pattern;
 
 // TODO(pawel): should we offer a multi-message parser capable of parsing multiple types of
 // messages?  E.g., it could be implemented as a composite trying out different parsers and using
@@ -31,9 +36,18 @@ import com.pinterest.secor.message.ParsedMessage;
  */
 public abstract class MessageParser {
     protected SecorConfig mConfig;
+    protected String[] mNestedFields;
+    private static final Logger LOG = LoggerFactory.getLogger(MessageParser.class);
 
     public MessageParser(SecorConfig config) {
         mConfig = config;
+        if (mConfig.getMessageTimestampName() != null &&
+                !mConfig.getMessageTimestampName().isEmpty() &&
+                mConfig.getMessageTimestampNameSeparator() != null &&
+                !mConfig.getMessageTimestampNameSeparator().isEmpty()) {
+            String separatorPattern = Pattern.quote(mConfig.getMessageTimestampNameSeparator());
+            mNestedFields = mConfig.getMessageTimestampName().split(separatorPattern);
+        }
     }
 
     public ParsedMessage parse(Message message) throws Exception {
@@ -43,4 +57,26 @@ public abstract class MessageParser {
     }
 
     public abstract String[] extractPartitions(Message payload) throws Exception;
+
+    public Object getJsonFieldValue(JSONObject jsonObject) {
+        Object fieldValue = null;
+        if (mNestedFields != null) {
+            Object finalValue = null;
+            for (int i=0; i < mNestedFields.length; i++) {
+                if (!jsonObject.containsKey(mNestedFields[i])) {
+                    LOG.warn("Could not find key {} in message", mConfig.getMessageTimestampName());
+                    break;
+                }
+                if (i < (mNestedFields.length -1)) {
+                    jsonObject = (JSONObject) jsonObject.get(mNestedFields[i]);
+                } else {
+                    finalValue = jsonObject.get(mNestedFields[i]);
+                }
+            }
+            fieldValue = finalValue;
+        } else {
+            fieldValue = jsonObject.get(mConfig.getMessageTimestampName());
+        }
+        return fieldValue;
+    }
 }
diff --git a/src/main/java/com/pinterest/secor/parser/PartitionFinalizer.java b/src/main/java/com/pinterest/secor/parser/PartitionFinalizer.java
index 558d0fd41dd01d399b265c2f83db74ac90f085d1..927381ec39906a0d9c06c0d32fbcbd94c811461a 100644
--- a/src/main/java/com/pinterest/secor/parser/PartitionFinalizer.java
+++ b/src/main/java/com/pinterest/secor/parser/PartitionFinalizer.java
@@ -22,6 +22,7 @@ import com.pinterest.secor.util.CompressionUtil;
 import com.pinterest.secor.util.FileUtil;
 import com.pinterest.secor.util.ReflectionUtil;
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.kafka.common.TopicPartition;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -67,8 +68,8 @@ public class PartitionFinalizer {
         Message message = mKafkaClient.getLastMessage(topicPartition);
         if (message == null) {
             // This will happen if no messages have been posted to the given topic partition.
-            LOG.error("No message found for topic " + topicPartition.getTopic() + " partition " +
-                topicPartition.getPartition());
+            LOG.error("No message found for topic {} partition {}", 
+                    topicPartition.topic(), topicPartition.partition());
             return -1;
         }
         return mMessageParser.extractTimestampMillis(message);
@@ -93,8 +94,8 @@ public class PartitionFinalizer {
     private long getCommittedTimestampMillis(TopicPartition topicPartition) throws Exception {
         Message message = mKafkaClient.getCommittedMessage(topicPartition);
         if (message == null) {
-            LOG.error("No message found for topic " + topicPartition.getTopic() + " partition " +
-                    topicPartition.getPartition());
+            LOG.error("No message found for topic {} partition {}",
+                    topicPartition.topic(), topicPartition.partition());
             return -1;
         }
         return mMessageParser.extractTimestampMillis(message);
@@ -164,11 +165,11 @@ public class PartitionFinalizer {
             try {
                 mQuboleClient.addPartition(mConfig.getHivePrefix() + topic, "dt='" + partitionStr + "'");
             } catch (Exception e) {
-                LOG.error("failed to finalize topic " + topic + " partition dt=" + partitionStr,
+                LOG.error("failed to finalize topic {} partition dt={}", topic, partitionStr,
                         e);
                 continue;
             }
-            LOG.info("touching file " + successFilePath);
+            LOG.info("touching file {}", successFilePath);
             FileUtil.touch(successFilePath);
         }
     }
@@ -199,8 +200,8 @@ public class PartitionFinalizer {
         for (int partition = 0; partition < numPartitions; ++partition) {
             TopicPartition topicPartition = new TopicPartition(topic, partition);
             long timestamp = getFinalizedTimestampMillis(topicPartition);
-            LOG.info("finalized timestamp for topic " + topic + " partition " + partition +
-                    " is " + timestamp);
+            LOG.info("finalized timestamp for topic {} partition {} is {}",
+                    topic, partition, timestamp);
             if (timestamp == -1) {
                 return -1;
             } else {
@@ -219,12 +220,12 @@ public class PartitionFinalizer {
         List<String> topics = mZookeeperConnector.getCommittedOffsetTopics();
         for (String topic : topics) {
             if (!topic.matches(mConfig.getKafkaTopicFilter())) {
-                LOG.info("skipping topic " + topic);
+                LOG.info("skipping topic {}", topic);
             } else {
-                LOG.info("finalizing topic " + topic);
+                LOG.info("finalizing topic {}", topic);
                 long finalizedTimestampMillis = getFinalizedTimestampMillis(topic);
-                LOG.info("finalized timestamp for topic " + topic + " is " +
-                        finalizedTimestampMillis);
+                LOG.info("finalized timestamp for topic {} is {}",
+                        topic, finalizedTimestampMillis);
                 if (finalizedTimestampMillis != -1) {
                     Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
                     calendar.setTimeInMillis(finalizedTimestampMillis);
diff --git a/src/main/java/com/pinterest/secor/parser/ThriftMessageParser.java b/src/main/java/com/pinterest/secor/parser/ThriftMessageParser.java
index d634baf445ccbe83d9e1e5d8d6d248c6f6cab715..9cd2100417984cf1f062a64ba16ad86d7a8fe4d4 100644
--- a/src/main/java/com/pinterest/secor/parser/ThriftMessageParser.java
+++ b/src/main/java/com/pinterest/secor/parser/ThriftMessageParser.java
@@ -53,6 +53,12 @@ public class ThriftMessageParser extends TimestampedMessageParser {
                 return mFieldName;
             }
         }
+        System.out.println("mDeserializer: " + mDeserializer);
+        System.out.println("message.getPayload(): " + message.getPayload());
+        System.out.println("mConfig: " + mConfig);
+        System.out.println("mConfig.getMessageTimestampName(): " + mConfig.getMessageTimestampName());
+        System.out.println("new ThriftTemplate(mConfig.getMessageTimestampName()): " + new ThriftTemplate(mConfig.getMessageTimestampName()));
+        System.out.println("all: " + mDeserializer.partialDeserializeI64(message.getPayload(),new ThriftTemplate(mConfig.getMessageTimestampName())));
         long timestamp = mDeserializer.partialDeserializeI64(message.getPayload(),
                 new ThriftTemplate(mConfig.getMessageTimestampName()));
         return toMillis(timestamp);
diff --git a/src/main/java/com/pinterest/secor/reader/MessageReader.java b/src/main/java/com/pinterest/secor/reader/MessageReader.java
index 12ccc22ff7152a58cbfa51684485da8919122b32..4bfa4383aa52fba72bccbed8c67973dc9fa487e7 100644
--- a/src/main/java/com/pinterest/secor/reader/MessageReader.java
+++ b/src/main/java/com/pinterest/secor/reader/MessageReader.java
@@ -18,28 +18,36 @@ package com.pinterest.secor.reader;
 
 import com.pinterest.secor.common.OffsetTracker;
 import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
+import com.pinterest.secor.common.ZookeeperConnector;
 import com.pinterest.secor.message.Message;
 import com.pinterest.secor.util.IdUtil;
 import com.pinterest.secor.util.RateLimitUtil;
 import com.pinterest.secor.util.StatsUtil;
-import kafka.consumer.Consumer;
-import kafka.consumer.ConsumerConfig;
-import kafka.consumer.ConsumerIterator;
-import kafka.consumer.KafkaStream;
-import kafka.consumer.TopicFilter;
-import kafka.consumer.Whitelist;
-import kafka.javaapi.consumer.ConsumerConnector;
-import kafka.message.MessageAndMetadata;
+
+import org.apache.kafka.clients.CommonClientConfigs;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
+import org.apache.kafka.common.TopicPartition;
+import org.mortbay.log.Log;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.net.UnknownHostException;
+import java.lang.management.ManagementFactory;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
+import java.util.ArrayList;
 import java.util.Map;
+import java.util.Set;
+import java.util.Collection;
 import java.util.Properties;
+import java.util.regex.Pattern;
+import java.net.UnknownHostException;
 
 /**
  * Message reader consumer raw Kafka messages.
@@ -51,22 +59,31 @@ public class MessageReader {
 
     private SecorConfig mConfig;
     private OffsetTracker mOffsetTracker;
-    private ConsumerConnector mConsumerConnector;
-    private ConsumerIterator mIterator;
     private HashMap<TopicPartition, Long> mLastAccessTime;
+    private final int mTopicPartitionForgetSeconds;
+    private final int mCheckMessagesPerSecond;
+    private int mNMessages;
+
+    private KafkaConsumer<String, byte[]> mKafkaConsumer;
+    private Iterator<ConsumerRecord<String, byte[]>> mNewIterator;
+    private final int consumerTimeoutMs;
 
-    public MessageReader(SecorConfig config, OffsetTracker offsetTracker) throws
+    public MessageReader(SecorConfig config, OffsetTracker offsetTracker, KafkaConsumer<String, byte[]> kafkaConsumer) throws
             UnknownHostException {
         mConfig = config;
         mOffsetTracker = offsetTracker;
 
-        mConsumerConnector = Consumer.createJavaConsumerConnector(createConsumerConfig());
+        consumerTimeoutMs = mConfig.getConsumerTimeoutMs();
+        mTopicPartitionForgetSeconds = mConfig.getTopicPartitionForgetSeconds();
+        mCheckMessagesPerSecond = mConfig.getMessagesPerSecond() / mConfig.getConsumerThreads();
+        mKafkaConsumer = kafkaConsumer;
 
-        TopicFilter topicFilter = new Whitelist(mConfig.getKafkaTopicFilter());
-        List<KafkaStream<byte[], byte[]>> streams =
-            mConsumerConnector.createMessageStreamsByFilter(topicFilter);
-        KafkaStream<byte[], byte[]> stream = streams.get(0);
-        mIterator = stream.iterator();
+
+        String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
+        LOG.info("kafkaConsumer (pid: " + pid + ") consumer before poll");
+        mNewIterator = mKafkaConsumer.poll(consumerTimeoutMs).iterator();
+        LOG.info("kafkaConsumer (pid: " + pid + ") consumer polled");
+        
         mLastAccessTime = new HashMap<TopicPartition, Long>();
         StatsUtil.setLabel("secor.kafka.consumer.id", IdUtil.getConsumerId());
     }
@@ -78,7 +95,7 @@ public class MessageReader {
         while (iterator.hasNext()) {
             Map.Entry pair = (Map.Entry) iterator.next();
             long lastAccessTime = (Long) pair.getValue();
-            if (now - lastAccessTime > mConfig.getTopicPartitionForgetSeconds()) {
+            if (now - lastAccessTime > mTopicPartitionForgetSeconds) {
                 iterator.remove();
             }
         }
@@ -90,71 +107,47 @@ public class MessageReader {
             if (topicPartitions.length() > 0) {
                 topicPartitions.append(' ');
             }
-            topicPartitions.append(topicPartition.getTopic() + '/' +
-                                   topicPartition.getPartition());
+            topicPartitions.append(topicPartition.topic() + '/' +
+                                   topicPartition.partition());
         }
         StatsUtil.setLabel("secor.topic_partitions", topicPartitions.toString());
     }
 
-    private ConsumerConfig createConsumerConfig() throws UnknownHostException {
-        Properties props = new Properties();
-        props.put("zookeeper.connect", mConfig.getZookeeperQuorum() + mConfig.getKafkaZookeeperPath());
-        props.put("group.id", mConfig.getKafkaGroup());
-
-        props.put("zookeeper.session.timeout.ms",
-                  Integer.toString(mConfig.getZookeeperSessionTimeoutMs()));
-        props.put("zookeeper.sync.time.ms", Integer.toString(mConfig.getZookeeperSyncTimeMs()));
-        props.put("auto.commit.enable", "false");
-        // This option is required to make sure that messages are not lost for new topics and
-        // topics whose number of partitions has changed.
-        props.put("auto.offset.reset", "smallest");
-        props.put("consumer.timeout.ms", Integer.toString(mConfig.getConsumerTimeoutMs()));
-        props.put("consumer.id", IdUtil.getConsumerId());
-        if (mConfig.getRebalanceMaxRetries() != null &&
-            !mConfig.getRebalanceMaxRetries().isEmpty()) {
-            props.put("rebalance.max.retries", mConfig.getRebalanceMaxRetries());
-        }
-        if (mConfig.getRebalanceBackoffMs() != null &&
-            !mConfig.getRebalanceBackoffMs().isEmpty()) {
-            props.put("rebalance.backoff.ms", mConfig.getRebalanceBackoffMs());
-        }
-        if (mConfig.getSocketReceieveBufferBytes() != null &&
-            !mConfig.getSocketReceieveBufferBytes().isEmpty()) {
-            props.put("socket.receive.buffer.bytes", mConfig.getSocketReceieveBufferBytes());
-        }
-        if (mConfig.getFetchMessageMaxBytes() != null && !mConfig.getFetchMessageMaxBytes().isEmpty()) {
-            props.put("fetch.message.max.bytes", mConfig.getFetchMessageMaxBytes());
-        }
-        if (mConfig.getFetchMinBytes() != null && !mConfig.getFetchMinBytes().isEmpty()) {
-            props.put("fetch.min.bytes", mConfig.getFetchMinBytes());
-        }
-        if (mConfig.getFetchWaitMaxMs() != null && !mConfig.getFetchWaitMaxMs().isEmpty()) {
-            props.put("fetch.wait.max.ms", mConfig.getFetchWaitMaxMs());
-        }
-
-        return new ConsumerConfig(props);
-    }
 
     public boolean hasNext() {
-        return mIterator.hasNext();
+    	if (!mNewIterator.hasNext()) {
+            String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
+            LOG.info("kafkaConsumer (pid: " + pid + ") consumer before poll");
+    		mNewIterator = mKafkaConsumer.poll(consumerTimeoutMs).iterator();
+            LOG.info("kafkaConsumer (pid: " + pid + ") consumer polled");
+    	}
+        return mNewIterator.hasNext();
     }
 
     public Message read() {
         assert hasNext();
-        RateLimitUtil.acquire();
-        MessageAndMetadata<byte[], byte[]> kafkaMessage = mIterator.next();
-        Message message = new Message(kafkaMessage.topic(), kafkaMessage.partition(),
-                                      kafkaMessage.offset(), kafkaMessage.message());
+        mNMessages = (mNMessages + 1) % mCheckMessagesPerSecond;
+        if (mNMessages % mCheckMessagesPerSecond == 0) {
+            RateLimitUtil.acquire(mCheckMessagesPerSecond);
+        }
+        
+        ConsumerRecord<String, byte[]> kafkaRecord = mNewIterator.next();
+        Message message = new Message(kafkaRecord.topic(), kafkaRecord.partition(),
+        		kafkaRecord.offset(), kafkaRecord.value());
         TopicPartition topicPartition = new TopicPartition(message.getTopic(),
                                                            message.getKafkaPartition());
         updateAccessTime(topicPartition);
+        
         // Skip already committed messages.
         long committedOffsetCount = mOffsetTracker.getTrueCommittedOffsetCount(topicPartition);
-        LOG.debug("read message" + message);
-        exportStats();
+        LOG.debug("read message {}", message);
+        if (mNMessages % mCheckMessagesPerSecond == 0) {
+            exportStats();
+        }
+
         if (message.getOffset() < committedOffsetCount) {
-            LOG.debug("skipping message message " + message + " because its offset precedes " +
-                      "committed offset count " + committedOffsetCount);
+            LOG.warn("skipping message message {} because its offset precedes " +
+                      "committed offset count {}", message, committedOffsetCount);
             return null;
         }
         return message;
diff --git a/src/main/java/com/pinterest/secor/supervisor/Supervisor.java b/src/main/java/com/pinterest/secor/supervisor/Supervisor.java
new file mode 100644
index 0000000000000000000000000000000000000000..a4a3ed4fcbfc1c2c2ef62cf975d6b1c7e4168260
--- /dev/null
+++ b/src/main/java/com/pinterest/secor/supervisor/Supervisor.java
@@ -0,0 +1,55 @@
+package com.pinterest.secor.supervisor;
+
+import com.pinterest.secor.common.SecorConfig;
+import com.pinterest.secor.common.SecorReports;
+import com.pinterest.secor.consumer.Consumer;
+import org.apache.kafka.common.TopicPartition;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+
+/**
+ * Created by guy on 8/22/16.
+ */
+public class Supervisor extends Thread {
+    private static final Logger LOG = LoggerFactory.getLogger(Supervisor.class);
+
+    private SecorConfig mConfig;
+    private List<Consumer> consumers;
+    private int hangsTimeout;
+    private final int checkTimeout = 500; //TODO: replace with something smart
+
+    public Supervisor(List<Consumer> consumers, SecorConfig config) {
+        this.mConfig = config;
+        this.consumers = consumers;
+        this.hangsTimeout = mConfig.getConsumerTimeoutMs()*2 + checkTimeout;
+    }
+
+    @Override
+    public void run() {
+        long[] consumersReportTime = new long[consumers.size()];
+        for (int idx=0 ; idx< consumers.size(); idx++) {
+            consumersReportTime[idx] = consumers.get(idx).getmReport().getTimestamp();
+        }
+        try {
+            while (true) {
+                Thread.sleep(hangsTimeout);
+                for (int idx=0 ; idx< consumers.size(); idx++) {
+                    Consumer consumer = consumers.get(idx);
+                    SecorReports report = consumer.getmReport();
+                    if (consumersReportTime[idx] == report.getTimestamp()) {
+                        if (report.isHangs()) {
+                            LOG.error("hangs status - " + report.getStatus());
+                            throw new Exception("consumer " + consumer.getId() + " hangs");
+                        }
+                    } else {
+                        consumersReportTime[idx] = report.getTimestamp();
+                    }
+                }
+            }
+        } catch (Exception e) {
+            throw new RuntimeException("Supervisor failed", e);
+        }
+    }
+}
diff --git a/src/main/java/com/pinterest/secor/tools/LogFileDeleter.java b/src/main/java/com/pinterest/secor/tools/LogFileDeleter.java
index 68535da859114b8ebd195b92d2ae671144c451de..78802d077b52ce154cb4d98b7f68bc751ff57a0e 100644
--- a/src/main/java/com/pinterest/secor/tools/LogFileDeleter.java
+++ b/src/main/java/com/pinterest/secor/tools/LogFileDeleter.java
@@ -48,13 +48,13 @@ public class LogFileDeleter {
         for (String consumerDir : consumerDirs) {
             long modificationTime = FileUtil.getModificationTimeMsRecursive(consumerDir);
             String modificationTimeStr = format.format(modificationTime);
-            LOG.info("Consumer log dir " + consumerDir + " last modified at " +
-                    modificationTimeStr);
+            LOG.info("Consumer log dir {} last modified at {}",
+                    consumerDir, modificationTimeStr);
             final long localLogDeleteAgeMs =
                     mConfig.getLocalLogDeleteAgeHours() * 60L * 60L * 1000L;
             if (System.currentTimeMillis() - modificationTime > localLogDeleteAgeMs) {
-                LOG.info("Deleting directory " + consumerDir + " last modified at " +
-                        modificationTimeStr);
+                LOG.info("Deleting directory {} last modified at {}",
+                        consumerDir, modificationTimeStr);
                 FileUtil.delete(consumerDir);
             }
         }
diff --git a/src/main/java/com/pinterest/secor/tools/LogFileVerifier.java b/src/main/java/com/pinterest/secor/tools/LogFileVerifier.java
index 531f29cbc2273e2a3edac82e9f937a399c3875b6..959a305c42947dbe5b8f84862a067c0bca7fbb8b 100644
--- a/src/main/java/com/pinterest/secor/tools/LogFileVerifier.java
+++ b/src/main/java/com/pinterest/secor/tools/LogFileVerifier.java
@@ -18,7 +18,6 @@ package com.pinterest.secor.tools;
 
 import com.pinterest.secor.common.LogFilePath;
 import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
 import com.pinterest.secor.io.FileReader;
 import com.pinterest.secor.io.KeyValue;
 import com.pinterest.secor.util.CompressionUtil;
@@ -26,6 +25,7 @@ import com.pinterest.secor.util.FileUtil;
 import com.pinterest.secor.util.ReflectionUtil;
 
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.kafka.common.TopicPartition;
 
 import java.io.IOException;
 import java.util.*;
@@ -49,7 +49,17 @@ public class LogFileVerifier {
     }
 
     private String getPrefix() {
-        return "s3n://" + mConfig.getS3Bucket() + "/" + mConfig.getS3Path();
+    	if (mConfig.getCloudService().equals("Swift")){
+        	String container = null;
+        	if (mConfig.getSeperateContainersForTopics()) {
+        		container = mTopic;
+        	} else {
+        		container = mConfig.getSwiftContainer();
+        	}
+        	return "swift2d://" + container + ".GENERICPROJECT/" + mConfig.getSwiftPath();
+        } else {
+        	return "s3n://" + mConfig.getS3Bucket() + "/" + mConfig.getS3Path();
+        }
     }
 
     private String getTopicPrefix() {
@@ -57,8 +67,9 @@ public class LogFileVerifier {
     }
 
     private void populateTopicPartitionToOffsetToFiles() throws IOException {
-        String prefix = getPrefix();
+    	String prefix = getPrefix();
         String topicPrefix = getTopicPrefix();
+        
         String[] paths = FileUtil.listRecursively(topicPrefix);
         for (String path : paths) {
             if (!path.endsWith("/_SUCCESS")) {
@@ -128,10 +139,10 @@ public class LogFileVerifier {
             SortedMap<Long, HashSet<LogFilePath>> offsetToFiles =
                 (SortedMap<Long, HashSet<LogFilePath>>) entry.getValue();
             for (HashSet<LogFilePath> logFilePaths : offsetToFiles.values()) {
-                int messageCount = 0;
+            	int messageCount = 0;
                 long offset = -2;
                 for (LogFilePath logFilePath : logFilePaths) {
-                    assert offset == -2 || offset == logFilePath.getOffset():
+                	assert offset == -2 || offset == logFilePath.getOffset():
                         Long.toString(offset) + " || " + offset + " == " + logFilePath.getOffset();
                     messageCount += getMessageCount(logFilePath);
                     offset = logFilePath.getOffset();
@@ -139,8 +150,8 @@ public class LogFileVerifier {
                 if (previousOffset != -2 && offset - previousOffset != previousMessageCount) {
                     TopicPartition topicPartition = (TopicPartition) entry.getKey();
                     throw new RuntimeException("Message count of " + previousMessageCount +
-                                               " in topic " + topicPartition.getTopic() +
-                                               " partition " + topicPartition.getPartition() +
+                                               " in topic " + topicPartition.topic() +
+                                               " partition " + topicPartition.partition() +
                                                " does not agree with adjacent offsets " +
                                                previousOffset + " and " + offset);
                 }
@@ -187,8 +198,8 @@ public class LogFileVerifier {
             for (Long offset : offsets) {
                 if (lastOffset != -2) {
                     assert lastOffset + 1 == offset: Long.toString(offset) + " + 1 == " + offset +
-                        " for topic " + topicPartition.getTopic() + " partition " +
-                        topicPartition.getPartition();
+                        " for topic " + topicPartition.topic() + " partition " +
+                        topicPartition.partition();
                 }
                 lastOffset = offset;
             }
@@ -214,4 +225,4 @@ public class LogFileVerifier {
         );
         return fileReader;
     }
-}
\ No newline at end of file
+}
diff --git a/src/main/java/com/pinterest/secor/tools/ProgressMonitor.java b/src/main/java/com/pinterest/secor/tools/ProgressMonitor.java
index 1e7c7850af771b20ed70326143f8c3c8f230b027..6b33eeb34b831bb421ff58b11ce3b411f96af5bc 100644
--- a/src/main/java/com/pinterest/secor/tools/ProgressMonitor.java
+++ b/src/main/java/com/pinterest/secor/tools/ProgressMonitor.java
@@ -21,7 +21,6 @@ import com.google.common.collect.Lists;
 import com.google.common.net.HostAndPort;
 import com.pinterest.secor.common.KafkaClient;
 import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
 import com.pinterest.secor.common.ZookeeperConnector;
 import com.pinterest.secor.message.Message;
 import com.pinterest.secor.parser.MessageParser;
@@ -36,6 +35,8 @@ import net.minidev.json.JSONValue;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.kafka.common.TopicPartition;
+
 import java.io.BufferedReader;
 import java.io.DataOutputStream;
 import java.io.IOException;
@@ -115,7 +116,7 @@ public class ProgressMonitor {
 
     private void exportToTsdb(Stat stat)
             throws IOException {
-        LOG.info("exporting metric to tsdb " + stat);
+        LOG.info("exporting metric to tsdb {}", stat);
         makeRequest(stat.toString());
     }
 
@@ -166,7 +167,7 @@ public class ProgressMonitor {
         for (String topic : topics) {
             if (topic.matches(mConfig.getMonitoringBlacklistTopics()) ||
                     !topic.matches(mConfig.getKafkaTopicFilter())) {
-                LOG.info("skipping topic " + topic);
+                LOG.info("skipping topic {}", topic);
                 continue;
             }
             List<Integer> partitions = mZookeeperConnector.getCommittedOffsetPartitions(topic);
@@ -176,8 +177,8 @@ public class ProgressMonitor {
                 long committedOffset = - 1;
                 long committedTimestampMillis = -1;
                 if (committedMessage == null) {
-                    LOG.warn("no committed message found in topic " + topic + " partition " +
-                        partition);
+                    LOG.warn("no committed message found in topic {} partition {}",
+                            topic, partition);
                 } else {
                     committedOffset = committedMessage.getOffset();
                     committedTimestampMillis = getTimestamp(committedMessage);
@@ -185,7 +186,7 @@ public class ProgressMonitor {
 
                 Message lastMessage = mKafkaClient.getLastMessage(topicPartition);
                 if (lastMessage == null) {
-                    LOG.warn("no message found in topic " + topic + " partition " + partition);
+                    LOG.warn("no message found in topic {} partition {}", topic, partition);
                 } else {
                     long lastOffset = lastMessage.getOffset();
                     long lastTimestampMillis = getTimestamp(lastMessage);
@@ -203,10 +204,10 @@ public class ProgressMonitor {
                     stats.add(Stat.createInstance("secor.lag.offsets", tags, Long.toString(offsetLag), timestamp));
                     stats.add(Stat.createInstance("secor.lag.seconds", tags, Long.toString(timestampMillisLag / 1000), timestamp));
 
-                    LOG.debug("topic " + topic + " partition " + partition + " committed offset " +
-                        committedOffset + " last offset " + lastOffset + " committed timestamp " +
-                            (committedTimestampMillis / 1000) + " last timestamp " +
-                            (lastTimestampMillis / 1000));
+                    LOG.debug("topic {} partition {} committed offset {}" +
+                            " last offset {} committed timestamp {} last timestamp {}",
+                            topic, partition, committedOffset, lastOffset, 
+                            (committedTimestampMillis / 1000), (lastTimestampMillis / 1000));
                 }
             }
         }
diff --git a/src/main/java/com/pinterest/secor/tools/RandomPartitioner.java b/src/main/java/com/pinterest/secor/tools/RandomPartitioner.java
index 7bf9db629d3c6195ebcc105dd90f610beedee104..61888c4861f2bc95e14e50f4b8e60e529415f1a5 100644
--- a/src/main/java/com/pinterest/secor/tools/RandomPartitioner.java
+++ b/src/main/java/com/pinterest/secor/tools/RandomPartitioner.java
@@ -16,9 +16,10 @@
  */
 package com.pinterest.secor.tools;
 
-import kafka.producer.Partitioner;
+import org.apache.kafka.common.Cluster;
+import org.apache.kafka.clients.producer.Partitioner;
 import kafka.utils.VerifiableProperties;
-
+import java.util.Map;
 /**
  * Random partitioner spreads messages evenly across partitions.
  *
@@ -27,10 +28,19 @@ import kafka.utils.VerifiableProperties;
 public class RandomPartitioner implements Partitioner {
     public RandomPartitioner(VerifiableProperties properties) {
     }
+    
+    public RandomPartitioner() {
+    }
 
-    public int partition(Object key, int numPartitions) {
+    public void configure(Map<String,?> configs){
+        
+    }
+    
+    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
         String stringKey = (String) key;
         int intKey = Integer.parseInt(stringKey);
-        return intKey % numPartitions;
+        return intKey % cluster.partitionCountForTopic(topic);
     }
+    
+    public void close(){}
 }
diff --git a/src/main/java/com/pinterest/secor/tools/TestLogMessageProducer.java b/src/main/java/com/pinterest/secor/tools/TestLogMessageProducer.java
index fee7070cd086436a021bd55c1d3605ecbc69693e..b57f675b8643622f2f33c5341389c5ad64a1c31e 100644
--- a/src/main/java/com/pinterest/secor/tools/TestLogMessageProducer.java
+++ b/src/main/java/com/pinterest/secor/tools/TestLogMessageProducer.java
@@ -16,9 +16,9 @@
  */
 package com.pinterest.secor.tools;
 
-import kafka.javaapi.producer.Producer;
-import kafka.producer.KeyedMessage;
-import kafka.producer.ProducerConfig;
+import org.apache.kafka.clients.producer.KafkaProducer;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.clients.CommonClientConfigs;
 
 import org.apache.thrift.TException;
 import org.apache.thrift.TSerializer;
@@ -30,6 +30,17 @@ import com.pinterest.secor.thrift.TestMessage;
 import com.pinterest.secor.thrift.TestEnum;
 
 import java.util.Properties;
+import java.util.Enumeration;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.UnsupportedEncodingException;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 
 /**
  * Test log message producer generates test messages and submits them to kafka.
@@ -37,6 +48,7 @@ import java.util.Properties;
  * @author Pawel Garbacki (pawel@pinterest.com)
  */
 public class TestLogMessageProducer extends Thread {
+    private static final Logger LOG = LoggerFactory.getLogger(TestLogMessageProducer.class);
     private final String mTopic;
     private final int mNumMessages;
     private final String mType;
@@ -48,16 +60,35 @@ public class TestLogMessageProducer extends Thread {
     }
 
     public void run() {
+        LOG.info("***   Inside TestLogMessageProducer");
+        Properties systemProperties = System.getProperties();
+        String propertiesFileName = systemProperties.getProperty("config");
         Properties properties = new Properties();
-        properties.put("metadata.broker.list", "localhost:9092");
-        properties.put("partitioner.class", "com.pinterest.secor.tools.RandomPartitioner");
-        properties.put("serializer.class", "kafka.serializer.DefaultEncoder");
-        properties.put("key.serializer.class", "kafka.serializer.StringEncoder");
-        properties.put("request.required.acks", "1");
-
-        ProducerConfig config = new ProducerConfig(properties);
-        Producer<String, byte[]> producer = new Producer<String, byte[]>(config);
-
+        if (propertiesFileName.isEmpty() || !propertiesFileName.contains("testprod")){
+            properties.put("bootstrap.servers", "localhost:9092");
+            properties.put("partitioner.class", "com.pinterest.secor.tools.RandomPartitioner");
+            properties.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
+            properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
+            properties.put("acks", "1");
+        } else {
+	        LOG.info("*** Loading properties from " + propertiesFileName);
+	        InputStream propertiesFileStream = null;
+	        try {
+	          propertiesFileStream = new FileInputStream(propertiesFileName);
+	          properties.load(propertiesFileStream);
+	        } catch(Exception ex) {
+	          ex.printStackTrace();
+	        } finally {
+	        if (null != propertiesFileStream) {
+	            try {
+	              propertiesFileStream.close();
+	            } catch (Exception e) {
+	              e.printStackTrace();
+	            }
+	          }
+	        }
+        }
+        KafkaProducer<String, byte[]> producer = new KafkaProducer<String, byte[]>(properties);
         TProtocolFactory protocol = null;
         if(mType.equals("json")) {
             protocol = new TSimpleJSONProtocol.Factory();
@@ -68,6 +99,15 @@ public class TestLogMessageProducer extends Thread {
         }
 
         TSerializer serializer = new TSerializer(protocol);
+        FileOutputStream hOutStream = null;
+        String dumpMsgTmpFileName = systemProperties.getProperty("dump.msg.tmp.file");
+        if(null != dumpMsgTmpFileName && !dumpMsgTmpFileName.isEmpty()) {
+	        try {
+	            hOutStream = new FileOutputStream(dumpMsgTmpFileName);
+	        } catch(FileNotFoundException fnf) {
+	        	fnf.printStackTrace();
+	        }
+        }
         for (int i = 0; i < mNumMessages; ++i) {
             TestMessage testMessage = new TestMessage(System.currentTimeMillis() * 1000000L + i,
                                                       "some_value_" + i);
@@ -78,14 +118,29 @@ public class TestLogMessageProducer extends Thread {
             }
             byte[] bytes;
             try {
-                bytes = serializer.serialize(testMessage);
+            	bytes = serializer.serialize(testMessage);
             } catch(TException e) {
-                throw new RuntimeException("Failed to serialize message " + testMessage, e);
+            	throw new RuntimeException("Failed to serialize message " + testMessage, e);
             }
-            KeyedMessage<String, byte[]> data = new KeyedMessage<String, byte[]>(
+            if (null != hOutStream) {
+	            try {
+	            	hOutStream.write(bytes);
+	            	hOutStream.write("\n".getBytes());
+	            } catch(IOException ioe) {
+	            	System.out.println("Failed to save a message in the file");
+	            }
+            }
+            ProducerRecord<String, byte[]> data = new ProducerRecord<String, byte[]>(
                 mTopic, Integer.toString(i), bytes);
             producer.send(data);
         }
+        try { 
+        	if (null != hOutStream) {
+        		hOutStream.close();
+        	}
+        } catch(IOException ioe) {
+        	System.out.println("Oh no");
+        }
         producer.close();
     }
-}
\ No newline at end of file
+}
diff --git a/src/main/java/com/pinterest/secor/uploader/Uploader.java b/src/main/java/com/pinterest/secor/uploader/Uploader.java
index 6f5650deb577621c34bb5fc89d881918d937dc63..149961997b7507de78c49cd3400ab2e6ac163f1b 100644
--- a/src/main/java/com/pinterest/secor/uploader/Uploader.java
+++ b/src/main/java/com/pinterest/secor/uploader/Uploader.java
@@ -25,15 +25,22 @@ import com.pinterest.secor.util.FileUtil;
 import com.pinterest.secor.util.IdUtil;
 import com.pinterest.secor.util.ReflectionUtil;
 
+import com.twitter.common.zookeeper.ZooKeeperClient;
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.OffsetAndMetadata;
+import org.apache.kafka.common.TopicPartition;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
+import java.net.UnknownHostException;
 import java.util.*;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
+import java.util.concurrent.TimeoutException;
+
 
 /**
  * Uploader applies a set of policies to determine if any of the locally stored files should be
@@ -49,59 +56,82 @@ public class Uploader {
     private SecorConfig mConfig;
     private OffsetTracker mOffsetTracker;
     private FileRegistry mFileRegistry;
+    private KafkaConsumer<String, byte[]> mKafkaConsumer;
     private ZookeeperConnector mZookeeperConnector;
 
-    public Uploader(SecorConfig config, OffsetTracker offsetTracker, FileRegistry fileRegistry) {
-        this(config, offsetTracker, fileRegistry, new ZookeeperConnector(config));
+    public Uploader(SecorConfig config, OffsetTracker offsetTracker, FileRegistry fileRegistry, KafkaConsumer<String, byte[]> consumer) throws UnknownHostException, ZooKeeperClient.ZooKeeperConnectionException, InterruptedException, TimeoutException {
+        this(config, offsetTracker, fileRegistry, new ZookeeperConnector(config), consumer);
     }
 
     // For testing use only.
     public Uploader(SecorConfig config, OffsetTracker offsetTracker, FileRegistry fileRegistry,
-                    ZookeeperConnector zookeeperConnector) {
+                    ZookeeperConnector zookeeperConnector, KafkaConsumer<String, byte[]> kafkaConsumer) {
         mConfig = config;
         mOffsetTracker = offsetTracker;
         mFileRegistry = fileRegistry;
         mZookeeperConnector = zookeeperConnector;
+        mKafkaConsumer = kafkaConsumer;
     }
 
     private Future<?> upload(LogFilePath localPath) throws Exception {
-        String s3Prefix = "s3n://" + mConfig.getS3Bucket() + "/" + mConfig.getS3Path();
-        LogFilePath s3Path = new LogFilePath(s3Prefix, localPath.getTopic(),
+    	String prefix;
+        if (mConfig.getCloudService().equals("Swift")) {
+        	String container;
+        	if (mConfig.getSeperateContainersForTopics()) {
+        		if (!FileUtil.exists("swift2d://" + localPath.getTopic() + ".GENERICPROJECT/")){
+        			try {
+                        LOG.debug("creating container {}", localPath.getTopic());
+                    	FileUtil.CreateContainer(localPath.getTopic());
+                    } catch (IOException e) {
+                        throw new RuntimeException(e);
+                    }
+        		}
+        		container = localPath.getTopic();
+        	} else {
+        		container = mConfig.getSwiftContainer();
+        	}
+        	prefix = "swift2d://" + container + ".GENERICPROJECT/" + mConfig.getSwiftPath();
+    	} else {
+	        prefix = "s3n://" + mConfig.getS3Bucket() + "/" + mConfig.getS3Path();
+    	}
+        LogFilePath path = new LogFilePath(prefix, localPath.getTopic(),
                                              localPath.getPartitions(),
                                              localPath.getGeneration(),
                                              localPath.getKafkaPartition(),
                                              localPath.getOffset(),
                                              localPath.getExtension());
         final String localLogFilename = localPath.getLogFilePath();
-        final String s3LogFilename = s3Path.getLogFilePath();
-        LOG.info("uploading file " + localLogFilename + " to " + s3LogFilename);
+        final String logFilename = path.getLogFilePath();
+        LOG.info("uploading file {} to {}", localLogFilename, logFilename);
         return executor.submit(new Runnable() {
             @Override
             public void run() {
                 try {
-                    FileUtil.moveToS3(localLogFilename, s3LogFilename);
+               		FileUtil.moveToCloud(localLogFilename, logFilename);
                 } catch (IOException e) {
                     throw new RuntimeException(e);
                 }
             }
         });
+    	
     }
 
     private void uploadFiles(TopicPartition topicPartition) throws Exception {
         long committedOffsetCount = mOffsetTracker.getTrueCommittedOffsetCount(topicPartition);
         long lastSeenOffset = mOffsetTracker.getLastSeenOffset(topicPartition);
-        final String lockPath = "/secor/locks/" + topicPartition.getTopic() + "/" +
-                                topicPartition.getPartition();
-        // Deleting writers closes their streams flushing all pending data to the disk.
-        mFileRegistry.deleteWriters(topicPartition);
+        final String lockPath = "/secor/locks/" + topicPartition.topic() + "/" +
+                                topicPartition.partition();
         mZookeeperConnector.lock(lockPath);
         try {
             // Check if the committed offset has changed.
-            long zookeeperComittedOffsetCount = mZookeeperConnector.getCommittedOffsetCount(
-                    topicPartition);
-            if (zookeeperComittedOffsetCount == committedOffsetCount) {
-                LOG.info("uploading topic " + topicPartition.getTopic() + " partition " +
-                         topicPartition.getPartition());
+            OffsetAndMetadata offsetAndMetadat = mKafkaConsumer.committed(topicPartition);
+            long lastComittedOffset = offsetAndMetadat == null ? -1 : offsetAndMetadat.offset();
+
+            if (lastComittedOffset == committedOffsetCount) {
+                LOG.info("uploading topic {} partition {}",
+                        topicPartition.topic(), topicPartition.partition());
+                // Deleting writers closes their streams flushing all pending data to the disk.
+                mFileRegistry.deleteWriters(topicPartition);
                 Collection<LogFilePath> paths = mFileRegistry.getPaths(topicPartition);
                 List<Future<?>> uploadFutures = new ArrayList<Future<?>>();
                 for (LogFilePath path : paths) {
@@ -111,8 +141,11 @@ public class Uploader {
                     uploadFuture.get();
                 }
                 mFileRegistry.deleteTopicPartition(topicPartition);
-                mZookeeperConnector.setCommittedOffsetCount(topicPartition, lastSeenOffset + 1);
+                HashMap<TopicPartition, OffsetAndMetadata> committed = new HashMap<TopicPartition, OffsetAndMetadata>();
+                committed.put(topicPartition,new OffsetAndMetadata(lastSeenOffset + 1));
+                mKafkaConsumer.commitSync(committed);
                 mOffsetTracker.setCommittedOffsetCount(topicPartition, lastSeenOffset + 1);
+//                LogmetHandler.sendMetrics(topicPartition.toString() + ".offset", lastSeenOffset, mConfig);
             }
         } finally {
             mZookeeperConnector.unlock(lockPath);
@@ -173,10 +206,10 @@ public class Uploader {
         }
         mFileRegistry.deletePath(srcPath);
         if (dstPath == null) {
-            LOG.info("removed file " + srcPath.getLogFilePath());
+            LOG.info("removed file {}", srcPath.getLogFilePath());
         } else {
-            LOG.info("trimmed " + copiedMessages + " messages from " + srcPath.getLogFilePath() + " to " +
-                    dstPath.getLogFilePath() + " with start offset " + startOffset);
+            LOG.info("trimmed {} messages from {} to {} with start offset {}",
+                    copiedMessages, srcPath.getLogFilePath(), dstPath.getLogFilePath(), startOffset);
         }
     }
 
@@ -188,30 +221,31 @@ public class Uploader {
     }
 
     private void checkTopicPartition(TopicPartition topicPartition) throws Exception {
+
         final long size = mFileRegistry.getSize(topicPartition);
         final long modificationAgeSec = mFileRegistry.getModificationAgeSec(topicPartition);
-        if (size >= mConfig.getMaxFileSizeBytes() ||
+    	if (size >= mConfig.getMaxFileSizeBytes() ||
                 modificationAgeSec >= mConfig.getMaxFileAgeSeconds()) {
-            long newOffsetCount = mZookeeperConnector.getCommittedOffsetCount(topicPartition);
+            OffsetAndMetadata offsetAndMetadat = mKafkaConsumer.committed(topicPartition);
+            long newOffsetCount = offsetAndMetadat == null ? -1 : offsetAndMetadat.offset();
             long oldOffsetCount = mOffsetTracker.setCommittedOffsetCount(topicPartition,
                     newOffsetCount);
             long lastSeenOffset = mOffsetTracker.getLastSeenOffset(topicPartition);
-            if (oldOffsetCount == newOffsetCount) {
+
+        	if (oldOffsetCount == newOffsetCount) {
                 uploadFiles(topicPartition);
             } else if (newOffsetCount > lastSeenOffset) {  // && oldOffset < newOffset
-                LOG.debug("last seen offset " + lastSeenOffset +
-                          " is lower than committed offset count " + newOffsetCount +
-                          ".  Deleting files in topic " + topicPartition.getTopic() +
-                          " partition " + topicPartition.getPartition());
+                LOG.debug("last seen offset {} is lower than committed offset count {}." + 
+                          "  Deleting files in topic {} partition {}",
+                         lastSeenOffset, newOffsetCount, topicPartition.topic(), topicPartition.partition());
                 // There was a rebalancing event and someone committed an offset beyond that of the
                 // current message.  We need to delete the local file.
                 mFileRegistry.deleteTopicPartition(topicPartition);
             } else {  // oldOffsetCount < newOffsetCount <= lastSeenOffset
-                LOG.debug("previous committed offset count " + oldOffsetCount +
-                          " is lower than committed offset " + newOffsetCount +
-                          " is lower than or equal to last seen offset " + lastSeenOffset +
-                          ".  Trimming files in topic " + topicPartition.getTopic() +
-                          " partition " + topicPartition.getPartition());
+                LOG.debug("previous committed offset count {} is lower than committed offset {}" +
+                          " is lower than or equal to last seen offset {}." +
+                          " Trimming files in topic {} partition {}",oldOffsetCount, newOffsetCount, 
+                          lastSeenOffset, topicPartition.topic(), topicPartition.partition());
                 // There was a rebalancing event and someone committed an offset lower than that
                 // of the current message.  We need to trim local files.
                 trimFiles(topicPartition, newOffsetCount);
@@ -220,9 +254,10 @@ public class Uploader {
     }
 
     public void applyPolicy() throws Exception {
-        Collection<TopicPartition> topicPartitions = mFileRegistry.getTopicPartitions();
+    	Collection<TopicPartition> topicPartitions = mFileRegistry.getTopicPartitions();
         for (TopicPartition topicPartition : topicPartitions) {
             checkTopicPartition(topicPartition);
         }
+
     }
 }
diff --git a/src/main/java/com/pinterest/secor/util/CompressionUtil.java b/src/main/java/com/pinterest/secor/util/CompressionUtil.java
index c7d4bf7087283dcbc1e19ded990138a44395c27b..3f1b64d8b1ca4b02a4c7d1f5b214ea5332ffc2bd 100644
--- a/src/main/java/com/pinterest/secor/util/CompressionUtil.java
+++ b/src/main/java/com/pinterest/secor/util/CompressionUtil.java
@@ -19,6 +19,7 @@ package com.pinterest.secor.util;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import parquet.hadoop.codec.SnappyCodec;
 
 /**
  * 
@@ -33,6 +34,12 @@ public class CompressionUtil {
             throws Exception {
         CompressionCodecFactory ccf = new CompressionCodecFactory(
                 new Configuration());
-        return ccf.getCodecByClassName(className);
+        CompressionCodec codec = ccf.getCodecByClassName(className);
+        if (codec == null) {
+        	if (className.equals("parquet.hadoop.codec.SnappyCodec")) {
+        		codec = new SnappyCodec();
+        	}
+        }
+        return codec;
     }
 }
diff --git a/src/main/java/com/pinterest/secor/util/FileUtil.java b/src/main/java/com/pinterest/secor/util/FileUtil.java
index 22ba33ed357cbe51be5bdcf8f7c8bb202b83ede1..2cc80e24fda924bb8687cae707757fe4b5037086 100644
--- a/src/main/java/com/pinterest/secor/util/FileUtil.java
+++ b/src/main/java/com/pinterest/secor/util/FileUtil.java
@@ -22,10 +22,16 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
+import org.javaswift.joss.client.factory.AccountFactory;
+import org.javaswift.joss.client.factory.AccountConfig;
+import org.javaswift.joss.model.Account;
+
 import java.io.IOException;
 import java.net.URI;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Map;
+
 
 /**
  * File util implements utilities for interactions with the file system.
@@ -38,15 +44,29 @@ public class FileUtil {
     public static void configure(SecorConfig config) {
         mConfig = config;
     }
-
+    
     public static FileSystem getFileSystem(String path) throws IOException {
-        Configuration conf = new Configuration();
+    	Configuration conf = new Configuration();
         if (mConfig != null) {
-            conf.set("fs.s3n.awsAccessKeyId", mConfig.getAwsAccessKey());
-            conf.set("fs.s3n.awsSecretAccessKey", mConfig.getAwsSecretKey());
+        	if (mConfig.getCloudService().equals("Swift")){
+                conf.set("fs.swift2d.impl", "com.ibm.stocator.fs.ObjectStoreFileSystem");
+                conf.set("fs.swift2d.service.GENERICPROJECT.auth.method", mConfig.getSwiftAuthMethod());
+                conf.set("fs.swift2d.service.GENERICPROJECT.auth.url", mConfig.getSwiftAuthUrl());
+                conf.set("fs.swift2d.service.GENERICPROJECT.username", mConfig.getSwiftUsername());
+                conf.set("fs.swift2d.service.GENERICPROJECT.http.port", mConfig.getSwiftPort());
+                conf.set("fs.swift2d.service.GENERICPROJECT.public", mConfig.getSwiftPublic());
+                conf.set("fs.swift2d.service.GENERICPROJECT.endpoint.prefix", mConfig.getSwiftEndpointPrefix());
+                conf.set("fs.swift2d.service.GENERICPROJECT.region", mConfig.getSwiftRegion());
+              	conf.set("fs.swift2d.service.GENERICPROJECT.password", mConfig.getSwiftPassword());
+                conf.set("fs.swift2d.service.GENERICPROJECT.tenant", mConfig.getSwiftTenant());
+        	} else {
+	            conf.set("fs.s3n.awsAccessKeyId", mConfig.getAwsAccessKey());
+	            conf.set("fs.s3n.awsSecretAccessKey", mConfig.getAwsSecretKey());
+        	}
         }
         return FileSystem.get(URI.create(path), conf);
     }
+    
 
     public static String[] list(String path) throws IOException {
         FileSystem fs = getFileSystem(path);
@@ -54,9 +74,9 @@ public class FileUtil {
         ArrayList<String> paths = new ArrayList<String>();
         FileStatus[] statuses = fs.listStatus(fsPath);
         if (statuses != null) {
-            for (FileStatus status : statuses) {
-                Path statusPath = status.getPath();
-                if (path.startsWith("s3://") || path.startsWith("s3n://")) {
+        	for (FileStatus status : statuses) {
+            	Path statusPath = status.getPath();
+                if (path.startsWith("s3://") || path.startsWith("s3n://") || path.startsWith("swift2d://")) {
                     paths.add(statusPath.toUri().toString());
                 } else {
                     paths.add(statusPath.toUri().getPath());
@@ -97,18 +117,71 @@ public class FileUtil {
         }
     }
 
-    public static void moveToS3(String srcLocalPath, String dstS3Path) throws IOException {
+    // this function is used only when object extra metadata is supported
+    @Deprecated
+    private static void updateMetadata(Map<String, Object> metadata, String dstCloudPath){
+		AccountConfig config = new AccountConfig();
+		config.setUsername(mConfig.getSwiftUsername());
+		config.setAuthUrl(mConfig.getSwiftAuthUrl());
+		if (mConfig.getSwiftAuthMethod().equals("true")) {
+        	config.setPassword(mConfig.getSwiftApiKey());
+        } else {
+        	config.setPassword(mConfig.getSwiftPassword());
+    		config.setTenantName(mConfig.getSwiftTenant());
+        }
+		
+		Account account = new AccountFactory(config).createAccount();
+		String container = dstCloudPath.split("/")[2].split("\\.")[0];
+		int indexOfName = -1;
+		for (int i = 0; i < 3 ; i++) {
+			indexOfName = dstCloudPath.indexOf("/", indexOfName + 1);
+		}
+		while (dstCloudPath.charAt(++indexOfName) == '/') {} 
+		String objectName = dstCloudPath.substring( indexOfName );
+		account.getContainer(container).getObject(objectName).setMetadata(metadata);
+    }
+
+    public static void moveToCloud(String srcLocalPath, String dstCloudPath) throws IOException {
         Path srcPath = new Path(srcLocalPath);
-        Path dstPath = new Path(dstS3Path);
-        getFileSystem(dstS3Path).moveFromLocalFile(srcPath, dstPath);
+        Path dstPath = new Path(dstCloudPath);
+        FileSystem dstFs = getFileSystem(dstCloudPath);
+        dstFs.moveFromLocalFile(srcPath, dstPath);
+
+        // metadata - disabled for the moment
+//        FileSystem fs = FileUtil.getFileSystem(srcLocalPath);
+//        Path metadataPath = new Path(srcLocalPath + ".metadata");
+//        if (fs.exists(metadataPath)) {
+//        	try {
+//	        	InputStream is = fs.open(metadataPath);
+//	            ObjectInputStream ois = new ObjectInputStream(is);
+//	            Map<String,Object> metadata = (Map<String,Object>) ois.readObject();
+//	        	updateMetadata(metadata, dstCloudPath);
+//	        	ois.close();
+//	        	fs.delete(metadataPath, false);
+//	        	fs.close();
+//        	} catch (ClassNotFoundException e) {
+//        		throw new RuntimeException(e);
+//        	}
+//        }
     }
 
+
     public static void touch(String path) throws IOException {
         FileSystem fs = getFileSystem(path);
         Path fsPath = new Path(path);
         fs.create(fsPath).close();
     }
 
+    public static void CreateContainer(String topic) throws IOException {
+    	String containerUrl = "swift2d://" + topic + ".GENERICPROJECT"; 		
+		Path containerPath = new Path(containerUrl);
+		try {
+			getFileSystem(containerUrl).create(containerPath).close();
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        }
+    }
+    
     public static long getModificationTimeMsRecursive(String path) throws IOException {
         FileSystem fs = getFileSystem(path);
         Path fsPath = new Path(path);
@@ -119,7 +192,7 @@ public class FileUtil {
             for (FileStatus fileStatus : statuses) {
                 Path statusPath = fileStatus.getPath();
                 String stringPath;
-                if (path.startsWith("s3://") || path.startsWith("s3n://")) {
+                if (path.startsWith("s3://") || path.startsWith("s3n://") || path.startsWith("swift2d://")) {
                     stringPath = statusPath.toUri().toString();
                 } else {
                     stringPath = statusPath.toUri().getPath();
diff --git a/src/main/java/com/pinterest/secor/util/IdUtil.java b/src/main/java/com/pinterest/secor/util/IdUtil.java
index 122be30fff3f9de119d998748c0d43367e6c376c..a18c770fee5c0dc97bb066ae2c75b9c3ee5c8d16 100644
--- a/src/main/java/com/pinterest/secor/util/IdUtil.java
+++ b/src/main/java/com/pinterest/secor/util/IdUtil.java
@@ -16,6 +16,7 @@
  */
 package com.pinterest.secor.util;
 
+import java.lang.Math;
 import java.lang.management.ManagementFactory;
 import java.net.InetAddress;
 import java.net.UnknownHostException;
@@ -26,8 +27,15 @@ import java.net.UnknownHostException;
  * @author Pawel Garbacki (pawel@pinterest.com)
  */
 public class IdUtil {
-    public static String getConsumerId() throws UnknownHostException {
-        String hostname = InetAddress.getLocalHost().getHostName();
+    public static String getConsumerId() {
+        String hostname = "";
+        try {
+            hostname = InetAddress.getLocalHost().getHostName() + (int)(Math.random()*1000);
+        } catch (UnknownHostException e) {
+            System.out.println("Unknown Host Exception - catched");
+            e.printStackTrace();
+            hostname = "" + (int)(Math.random()*1000);
+        }
         String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
         long threadId = Thread.currentThread().getId();
         return hostname + "_" + pid + "_" + threadId;
diff --git a/src/main/java/com/pinterest/secor/util/RateLimitUtil.java b/src/main/java/com/pinterest/secor/util/RateLimitUtil.java
index 57cb4b5c9bc7946b3616409bb0c4e8ca3ed347f5..2e130fde2abaec4dfd22d4676b6e6c7c1a9b8959 100644
--- a/src/main/java/com/pinterest/secor/util/RateLimitUtil.java
+++ b/src/main/java/com/pinterest/secor/util/RateLimitUtil.java
@@ -35,7 +35,7 @@ public class RateLimitUtil {
         mRateLimiter = RateLimiter.create(config.getMessagesPerSecond());
     }
 
-    public static void acquire() {
-        mRateLimiter.acquire();
+    public static void acquire(int n) {
+        mRateLimiter.acquire(n);
     }
 }
diff --git a/src/main/java/com/pinterest/secor/writer/MessageWriter.java b/src/main/java/com/pinterest/secor/writer/MessageWriter.java
index 0b46058be484dcfa4e6bef2784d63b6d8df7d54f..aadc51fb13a0ccc44fe6487812d0b2bb1457652b 100644
--- a/src/main/java/com/pinterest/secor/writer/MessageWriter.java
+++ b/src/main/java/com/pinterest/secor/writer/MessageWriter.java
@@ -20,7 +20,6 @@ import com.pinterest.secor.common.FileRegistry;
 import com.pinterest.secor.common.LogFilePath;
 import com.pinterest.secor.common.OffsetTracker;
 import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
 import com.pinterest.secor.io.FileWriter;
 import com.pinterest.secor.io.KeyValue;
 import com.pinterest.secor.message.Message;
@@ -28,7 +27,11 @@ import com.pinterest.secor.message.ParsedMessage;
 import com.pinterest.secor.util.CompressionUtil;
 import com.pinterest.secor.util.IdUtil;
 import com.pinterest.secor.util.StatsUtil;
+
 import org.apache.hadoop.io.compress.CompressionCodec;
+
+import org.apache.kafka.common.TopicPartition;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -48,6 +51,7 @@ public class MessageWriter {
     private String mFileExtension;
     private CompressionCodec mCodec;
     private String mLocalPrefix;
+    private final int mGeneration;
 
     public MessageWriter(SecorConfig config, OffsetTracker offsetTracker,
                          FileRegistry fileRegistry) throws Exception {
@@ -61,6 +65,7 @@ public class MessageWriter {
             mFileExtension = "";
         }
         mLocalPrefix = mConfig.getLocalPath() + '/' + IdUtil.getLocalMessageDir();
+        mGeneration = mConfig.getGeneration();
     }
 
     public void adjustOffset(Message message) throws IOException {
@@ -68,26 +73,25 @@ public class MessageWriter {
                                                            message.getKafkaPartition());
         long lastSeenOffset = mOffsetTracker.getLastSeenOffset(topicPartition);
         if (message.getOffset() != lastSeenOffset + 1) {
-            StatsUtil.incr("secor.consumer_rebalance_count." + topicPartition.getTopic());
+            StatsUtil.incr("secor.consumer_rebalance_count." + topicPartition.topic());
             // There was a rebalancing event since we read the last message.
-            LOG.debug("offset of message " + message +
-                      " does not follow sequentially the last seen offset " + lastSeenOffset +
-                      ".  Deleting files in topic " + topicPartition.getTopic() + " partition " +
-                      topicPartition.getPartition());
+            LOG.debug("offset of message {} does not follow sequentially" +
+                   " the last seen offset {}.  Deleting files in topic {} partition {}",
+                   message, lastSeenOffset, topicPartition.topic(), topicPartition.partition());
             mFileRegistry.deleteTopicPartition(topicPartition);
         }
         mOffsetTracker.setLastSeenOffset(topicPartition, message.getOffset());
     }
 
     public void write(ParsedMessage message) throws Exception {
-        TopicPartition topicPartition = new TopicPartition(message.getTopic(),
+    	TopicPartition topicPartition = new TopicPartition(message.getTopic(),
                                                            message.getKafkaPartition());
         long offset = mOffsetTracker.getAdjustedCommittedOffsetCount(topicPartition);
-        LogFilePath path = new LogFilePath(mLocalPrefix, mConfig.getGeneration(), offset, message,
+        LogFilePath path = new LogFilePath(mLocalPrefix, mGeneration, offset, message,
         		mFileExtension);
         FileWriter writer = mFileRegistry.getOrCreateWriter(path, mCodec);
-        writer.write(new KeyValue(message.getOffset(), message.getPayload()));
-        LOG.debug("appended message " + message + " to file " + path.getLogFilePath() +
-                  ".  File length " + writer.getLength());
+    	writer.write(new KeyValue(message.getOffset(), message.getPayload()));
+        LOG.debug("appended message {} to file {}.  File length {}", 
+                message, path, writer.getLength());
     }
 }
diff --git a/src/main/scripts/run_tests.sh b/src/main/scripts/run_tests.sh
index 6a021ee320776fc6ac41255705bca45953ec0ecc..3b174a125dd8e5f3f64fc2c52bca6ca759d2c444 100755
--- a/src/main/scripts/run_tests.sh
+++ b/src/main/scripts/run_tests.sh
@@ -1,5 +1,4 @@
 #!/usr/bin/env bash
-
 # Licensed to the Apache Software Foundation (ASF) under one or more
 # contributor license agreements.  See the NOTICE file distributed with
 # this work for additional information regarding copyright ownership.
@@ -34,10 +33,14 @@
 
 # Author: Pawel Garbacki (pawel@pinterest.com)
 
+
 PARENT_DIR=/tmp/secor_dev
 LOGS_DIR=${PARENT_DIR}/logs
 BUCKET=${SECOR_BUCKET:-test-bucket}
 S3_LOGS_DIR=s3://${BUCKET}/secor_dev
+SWIFT_CONTAINER=logsContainer
+# Should match the secor.swift.containers.for.each.topic value
+CONTAINER_PER_TOPIC=false
 MESSAGES=1000
 MESSAGE_TYPE=binary
 # For the compression tests to work, set this to the path of the Hadoop native libs.
@@ -45,19 +48,49 @@ HADOOP_NATIVE_LIB_PATH=lib
 # by default additional opts is empty
 ADDITIONAL_OPTS=
 
+# if you are testing using swift, fill the settings below
+
+# The variables in the block below are Bluemix oriented.
+# Check the variables in the next block for other scenarios.
+export OS_USER_ID=
+export OS_PASSWORD=
+export OS_PROJECT_ID=
+export OS_AUTH_URL=
+export OS_REGION_NAME=
+export OS_IDENTITY_API_VERSION=3
+export OS_AUTH_VERSION=3
+export PYTHONWARNINGS="ignore"
+
+# These variables are for not-Bluemix scenarios
+# Fill the data and uncomment.
+#SWIFT_USERNAME=
+#SWIFT_PASSWORD=
+#SWIFT_TENANT_NAME=
+#SWIFT_AUTH_URL=
+#SWIFT_SETTINGS="--os-username ${SWIFT_USERNAME} --os-password=${SWIFT_PASSWORD} --os-tenant-name=${SWIFT_TENANT_NAME} --os-auth-url=${SWIFT_AUTH_URL}"
+
 # various reader writer options to be used for testing
 declare -A READER_WRITERS
 READER_WRITERS[json]=com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory
+#READER_WRITERS[parquet]=com.pinterest.secor.io.impl.ParquetFileReaderWriterFactory #in order to test this you will need to upload the avro schema for the test data
 READER_WRITERS[binary]=com.pinterest.secor.io.impl.SequenceFileReaderWriterFactory
 
+
 # The minimum wait time is one minute plus delta.  Secor is configured to upload files older than
 # one minute and we need to make sure that everything ends up on s3 before starting verification.
 WAIT_TIME=${SECOR_WAIT_TIME:-120}
 BASE_DIR=$(dirname $0)
 CONF_DIR=${BASE_DIR}/..
 
+
+cloudService="s3"
+if [ "$#" != "0" ]; then
+   cloudService=${1}
+fi
+
 source ${BASE_DIR}/run_common.sh
 
+
 run_command() {
     echo "running $@"
     eval "$@"
@@ -73,10 +106,19 @@ check_for_native_libs() {
 
 recreate_dirs() {
     run_command "rm -r -f ${PARENT_DIR}"
-    if [ -n ${SECOR_LOCAL_S3} ]; then
-        run_command "s3cmd -c ${CONF_DIR}/test.s3cfg ls ${S3_LOGS_DIR} | awk '{ print \$4 }' | xargs -L 1 s3cmd -c ${CONF_DIR}/test.s3cfg del"
+
+    if [ ${cloudService} = "swift" ]; then
+	if ${CONTAINER_PER_TOPIC}; then
+	   run_command "swift delete test ${SWIFT_SETTINGS}"
+        else
+           run_command "swift delete ${SWIFT_CONTAINER} ${SWIFT_SETTINGS}"
+           sleep 3
+           run_command "swift post ${SWIFT_CONTAINER} ${SWIFT_SETTINGS}"
+        fi
     else
-        run_command "s3cmd del --recursive ${S3_LOGS_DIR}"
+        if [ -n ${SECOR_LOCAL_S3} ]; then
+            run_command "s3cmd -c ${CONF_DIR}/test.s3cfg ls ${S3_LOGS_DIR} | awk '{ print \$4 }' | xargs -L 1 s3cmd -c ${CONF_DIR}/test.s3cfg del"
+        fi
     fi
     # create logs directory
     if [ ! -d ${LOGS_DIR} ]; then
@@ -172,20 +214,29 @@ verify() {
     done
 }
 
+
+set_offsets_in_kafka() {
+    for group in secor_partition secor_backup; do
+        for partition in 0 1; do
+            run_command "${JAVA} -server -ea -Dlog4j.configuration=log4j.dev.properties \
+                -Dconfig=secor.test.backup.properties -cp ${CLASSPATH} \
+                com.pinterest.secor.main.TestSetOffsetMain -t test -g ${group} -o $1 -p ${partition} > \
+                ${LOGS_DIR}/set_offset_in_kafka.log 2>&1"
+        done
+    done
+}
+
 set_offsets_in_zookeeper() {
     for group in secor_backup secor_partition; do
         for partition in 0 1; do
-            run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 create \
-                /consumers \'\' > ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
-            run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 create \
-                /consumers/${group} \'\' > ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
-            run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 create \
-                /consumers/${group}/offsets \'\' > ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
-            run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 create \
-                /consumers/${group}/offsets/test \'\' > ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
-            run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 create \
-                /consumers/${group}/offsets/test/${partition} $1 > \
-                ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
+            cat <<EOF | run_command "${BASE_DIR}/run_zookeeper_command.sh localhost:2181 > ${LOGS_DIR}/run_zookeeper_command.log 2>&1"
+create /consumers ''
+create /consumers/${group} ''
+create /consumers/${group}/offsets ''
+create /consumers/${group}/offsets/test ''
+create /consumers/${group}/offsets/test/${partition} $1
+quit
+EOF
         done
     done
 }
@@ -223,17 +274,38 @@ post_and_verify_test() {
     echo "Waiting ${WAIT_TIME} sec for Secor to upload logs to s3"
     sleep ${WAIT_TIME}
     verify ${MESSAGES}
-
+    
     stop_all
     echo -e "\e[1;42;97mpost_and_verify_test succeeded\e[0m"
 }
 
+# Post some messages and verify that they are correctly processed.
+post_crash_and_verify_test() {
+    echo "running post_crash_and_verify_test"
+    initialize
+
+    start_secor
+    sleep 3
+    post_messages ${MESSAGES}
+    echo "Waiting few seconds for Secor grab messages from kafka and then restarting it"
+    sleep 8
+    stop_secor
+    sleep 1
+    start_secor
+    echo "Waiting ${WAIT_TIME} sec for Secor to upload logs to s3"
+    sleep ${WAIT_TIME}
+    verify ${MESSAGES}
+    
+    stop_all
+    echo -e "\e[1;42;97mpost_crash_and_verify_test succeeded\e[0m"
+}
+
 # Adjust offsets so that Secor consumes only half of the messages.
 start_from_non_zero_offset_test() {
     echo "running start_from_non_zero_offset_test"
     initialize
 
-    set_offsets_in_zookeeper $((${MESSAGES}/4))
+    set_offsets_in_kafka $((${MESSAGES}/4))
     post_messages ${MESSAGES}
     start_secor
     echo "Waiting ${WAIT_TIME} sec for Secor to upload logs to s3"
@@ -253,7 +325,7 @@ move_offset_back_test() {
     start_secor
     sleep 3
     post_messages $((${MESSAGES}/10))
-    set_offsets_in_zookeeper 2
+    set_offsets_in_kafka 2
     post_messages $((${MESSAGES}*9/10))
 
     echo "Waiting ${WAIT_TIME} sec for Secor to upload logs to s3"
@@ -285,13 +357,20 @@ post_and_verify_compressed_test() {
 }
 
 check_for_native_libs
-start_s3
+if [ ${cloudService} = "s3" ]; then
+    start_s3
+fi
 
 for key in ${!READER_WRITERS[@]}; do
-   MESSAGE_TYPE=${key}
+   if [ ${key} = "parquet" ]; then
+      MESSAGE_TYPE="json"
+   else
+      MESSAGE_TYPE=${key}
+   fi
    ADDITIONAL_OPTS=-Dsecor.file.reader.writer.factory=${READER_WRITERS[${key}]}
    echo "Running tests for Message Type: ${MESSAGE_TYPE} and ReaderWriter: ${READER_WRITERS[${key}]}"
    post_and_verify_test
+   post_crash_and_verify_test
    start_from_non_zero_offset_test
    move_offset_back_test
    if [ ${key} = "json" ]; then
@@ -303,4 +382,6 @@ for key in ${!READER_WRITERS[@]}; do
    fi
 done
 
-stop_s3
+if [ ${cloudService} = "s3" ]; then
+    stop_s3
+fi
diff --git a/src/main/scripts/run_zookeeper_command.sh b/src/main/scripts/run_zookeeper_command.sh
index 178b92adab6e8c2a2da3a4e130d287e7a8677162..873737b9512686da884850929f563e5fbb32c6d3 100755
--- a/src/main/scripts/run_zookeeper_command.sh
+++ b/src/main/scripts/run_zookeeper_command.sh
@@ -17,11 +17,6 @@
 
 # Author: Pawel Garbacki (pawel@pinterest.com)
 
-if [ $# -lt 3 ]; then
-    echo "USAGE: $0 zookeeper_host:port cmd args"
-    exit 1
-fi
-
 CURR_DIR=`dirname $0`
 source ${CURR_DIR}/run_common.sh
 
diff --git a/src/test/java/com/pinterest/secor/common/FileRegistryTest.java b/src/test/java/com/pinterest/secor/common/FileRegistryTest.java
index 021437ccab9f0a17b4ceada94c107627fda9acea..a84acac6ab5fb3b299ef12ba68814c9997e2b7c4 100644
--- a/src/test/java/com/pinterest/secor/common/FileRegistryTest.java
+++ b/src/test/java/com/pinterest/secor/common/FileRegistryTest.java
@@ -33,6 +33,8 @@ import org.powermock.modules.junit4.PowerMockRunner;
 
 import java.util.Collection;
 
+import org.apache.kafka.common.TopicPartition;
+
 /**
  * FileRegistryTest tests the file registry logic.
  *
diff --git a/src/test/java/com/pinterest/secor/parser/DateMessageParserTest.java b/src/test/java/com/pinterest/secor/parser/DateMessageParserTest.java
index 1766f2730ba94de4f98d092d0504f37313af24cc..1d5efe9cd8205d69fbd741d52fad2cbdbdc797f9 100644
--- a/src/test/java/com/pinterest/secor/parser/DateMessageParserTest.java
+++ b/src/test/java/com/pinterest/secor/parser/DateMessageParserTest.java
@@ -35,12 +35,13 @@ public class DateMessageParserTest extends TestCase {
     private Message mFormat2;
     private Message mFormat3;
     private Message mInvalidDate;
+    private Message mISOFormat;
+    private Message mNestedISOFormat;
     private OngoingStubbing<String> getTimestamp;
 
     @Override
     public void setUp() throws Exception {
         mConfig = Mockito.mock(SecorConfig.class);
-        Mockito.when(mConfig.getMessageTimestampName()).thenReturn("timestamp");
 
         byte format1[] = "{\"timestamp\":\"2014-07-30 10:53:20\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
                 .getBytes("UTF-8");
@@ -57,31 +58,39 @@ public class DateMessageParserTest extends TestCase {
         byte invalidDate[] = "{\"timestamp\":\"11111111\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
                 .getBytes("UTF-8");
         mInvalidDate = new Message("test", 0, 0, invalidDate);
-        
-        getTimestamp = Mockito.when(mConfig.getMessageTimestampInputPattern());
+
+        byte isoFormat[] = "{\"timestamp\":\"2016-01-11T11:50:28.647Z\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mISOFormat = new Message("test", 0, 0, isoFormat);
+
+        byte nestedISOFormat[] = "{\"meta_data\":{\"created\":\"2016-01-11T11:50:28.647Z\"},\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mNestedISOFormat = new Message("test", 0, 0, nestedISOFormat);
     }
 
     @Test
     public void testExtractDateUsingInputPattern() throws Exception {
-        getTimestamp.thenReturn("yyyy-MM-dd HH:mm:ss");
+        Mockito.when(mConfig.getMessageTimestampName()).thenReturn("timestamp");
+
+        Mockito.when(mConfig.getMessageTimestampInputPattern()).thenReturn("yyyy-MM-dd HH:mm:ss");
         assertEquals("dt=2014-07-30", new DateMessageParser(mConfig).extractPartitions(mFormat1)[0]);
 
-        getTimestamp.thenReturn("yyyy/MM/d");
+        Mockito.when(mConfig.getMessageTimestampInputPattern()).thenReturn("yyyy/MM/d");
         assertEquals("dt=2014-10-25", new DateMessageParser(mConfig).extractPartitions(mFormat2)[0]);
 
-        getTimestamp.thenReturn("yyyyy.MMMMM.dd GGG hh:mm aaa");
+        Mockito.when(mConfig.getMessageTimestampInputPattern()).thenReturn("yyyyy.MMMMM.dd GGG hh:mm aaa");
         assertEquals("dt=2001-07-04", new DateMessageParser(mConfig).extractPartitions(mFormat3)[0]);
     }
 
     @Test
     public void testExtractDateWithWrongEntries() throws Exception {
         // invalid date
-        getTimestamp.thenReturn("yyyy-MM-dd HH:mm:ss"); // any pattern
+        Mockito.when(mConfig.getMessageTimestampInputPattern()).thenReturn("yyyy-MM-dd HH:mm:ss"); // any pattern
         assertEquals(DateMessageParser.defaultDate, new DateMessageParser(
                 mConfig).extractPartitions(mInvalidDate)[0]);
 
         // invalid pattern
-        getTimestamp.thenReturn("yyy-MM-dd :s");
+        Mockito.when(mConfig.getMessageTimestampInputPattern()).thenReturn("yyy-MM-dd :s");
         assertEquals(DateMessageParser.defaultDate, new DateMessageParser(
                 mConfig).extractPartitions(mFormat1)[0]);
     }
diff --git a/src/test/java/com/pinterest/secor/parser/Iso8601ParserTest.java b/src/test/java/com/pinterest/secor/parser/Iso8601ParserTest.java
new file mode 100644
index 0000000000000000000000000000000000000000..269dc44db85e2187712c6bd800d78d7e7c32c1d1
--- /dev/null
+++ b/src/test/java/com/pinterest/secor/parser/Iso8601ParserTest.java
@@ -0,0 +1,95 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.pinterest.secor.parser;
+
+import junit.framework.TestCase;
+
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mockito;
+import org.mockito.stubbing.OngoingStubbing;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+import com.pinterest.secor.common.SecorConfig;
+import com.pinterest.secor.message.Message;
+
+@RunWith(PowerMockRunner.class)
+public class Iso8601ParserTest extends TestCase {
+
+    private SecorConfig mConfig;
+    private Message mFormat1;
+    private Message mFormat2;
+    private Message mFormat3;
+    private Message mFormat4;
+    private Message mInvalidDate;
+    private Message mISOFormat;
+    private Message mNestedISOFormat;
+    private Message mMissingDate;
+    private OngoingStubbing<String> getTimestamp;
+
+    @Override
+    public void setUp() throws Exception {
+        mConfig = Mockito.mock(SecorConfig.class);
+        byte format1[] = "{\"timestamp\":\"2014-07-30T10:53:20.001Z\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mFormat1 = new Message("test", 0, 0, format1);
+
+        byte format2[] = "{\"timestamp\":\"2014-07-29T10:53:20Z\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mFormat2 = new Message("test", 0, 0, format2);
+
+        byte format3[] = "{\"timestamp\":\"2001-07-04Z\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mFormat3 = new Message("test", 0, 0, format3);
+
+        byte format4[] = "{\"timestamp\":\"2016-03-02T18:36:14+00:00\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mFormat4 = new Message("test", 0, 0, format4);
+
+        byte nestedISOFormat[] = "{\"meta_data\":{\"created\":\"2016-01-11T11:50:28.647Z\"},\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mNestedISOFormat = new Message("test", 0, 0, nestedISOFormat);
+
+        byte invalidDate[] = "{\"timestamp\":\"111-11111111\",\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mInvalidDate = new Message("test", 0, 0, invalidDate);
+
+        byte missingDate[] = "{\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}"
+                .getBytes("UTF-8");
+        mMissingDate = new Message("test", 0, 0, missingDate);
+    }
+
+    @Test
+    public void testExtractDate() throws Exception {
+        Mockito.when(mConfig.getMessageTimestampName()).thenReturn("timestamp");
+
+        assertEquals("dt=2014-07-30", new Iso8601MessageParser(mConfig).extractPartitions(mFormat1)[0]);
+        assertEquals("dt=2014-07-29", new Iso8601MessageParser(mConfig).extractPartitions(mFormat2)[0]);
+        assertEquals("dt=2001-07-04", new Iso8601MessageParser(mConfig).extractPartitions(mFormat3)[0]);
+        assertEquals("dt=2016-03-02", new Iso8601MessageParser(mConfig).extractPartitions(mFormat4)[0]);
+        assertEquals("dt=1970-01-01", new Iso8601MessageParser(mConfig).extractPartitions(mInvalidDate)[0]);
+        assertEquals("dt=1970-01-01", new Iso8601MessageParser(mConfig).extractPartitions(mMissingDate)[0]);
+    }
+
+    @Test
+    public void testNestedField() throws Exception {
+        Mockito.when(mConfig.getMessageTimestampNameSeparator()).thenReturn(".");
+        Mockito.when(mConfig.getMessageTimestampName()).thenReturn("meta_data.created");
+
+        assertEquals("dt=2016-01-11", new Iso8601MessageParser(mConfig).extractPartitions(mNestedISOFormat)[0]);
+    }
+}
diff --git a/src/test/java/com/pinterest/secor/parser/JsonMessageParserTest.java b/src/test/java/com/pinterest/secor/parser/JsonMessageParserTest.java
index b288c8e557710962a6a5622938c5f7a2f36f1a9b..ad4d4080b3a901466258b29bef654054b01bdce9 100644
--- a/src/test/java/com/pinterest/secor/parser/JsonMessageParserTest.java
+++ b/src/test/java/com/pinterest/secor/parser/JsonMessageParserTest.java
@@ -32,6 +32,7 @@ public class JsonMessageParserTest extends TestCase {
     private Message mMessageWithMillisTimestamp;
     private Message mMessageWithMillisFloatTimestamp;
     private Message mMessageWithoutTimestamp;
+    private Message mMessageWithNestedTimestamp;
 
     @Override
     public void setUp() throws Exception {
@@ -53,6 +54,10 @@ public class JsonMessageParserTest extends TestCase {
         byte messageWithoutTimestamp[] =
                 "{\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}".getBytes("UTF-8");
         mMessageWithoutTimestamp = new Message("test", 0, 0, messageWithoutTimestamp);
+
+        byte messageWithNestedTimestamp[] =
+                "{\"meta_data\":{\"created\":\"1405911096123\"},\"id\":0,\"guid\":\"0436b17b-e78a-4e82-accf-743bf1f0b884\",\"isActive\":false,\"balance\":\"$3,561.87\",\"picture\":\"http://placehold.it/32x32\",\"age\":23,\"eyeColor\":\"green\",\"name\":\"Mercedes Brewer\",\"gender\":\"female\",\"company\":\"MALATHION\",\"email\":\"mercedesbrewer@malathion.com\",\"phone\":\"+1 (848) 471-3000\",\"address\":\"786 Gilmore Court, Brule, Maryland, 3200\",\"about\":\"Quis nostrud Lorem deserunt esse ut reprehenderit aliqua nisi et sunt mollit est. Cupidatat incididunt minim anim eiusmod culpa elit est dolor ullamco. Aliqua cillum eiusmod ullamco nostrud Lorem sit amet Lorem aliquip esse esse velit.\\r\\n\",\"registered\":\"2014-01-14T13:07:28 +08:00\",\"latitude\":47.672012,\"longitude\":102.788623,\"tags\":[\"amet\",\"amet\",\"dolore\",\"eu\",\"qui\",\"fugiat\",\"laborum\"],\"friends\":[{\"id\":0,\"name\":\"Rebecca Hardy\"},{\"id\":1,\"name\":\"Sutton Briggs\"},{\"id\":2,\"name\":\"Dena Campos\"}],\"greeting\":\"Hello, Mercedes Brewer! You have 7 unread messages.\",\"favoriteFruit\":\"strawberry\"}".getBytes("UTF-8");
+        mMessageWithNestedTimestamp = new Message("test", 0, 0, messageWithNestedTimestamp);
     }
 
     @Test
@@ -68,6 +73,15 @@ public class JsonMessageParserTest extends TestCase {
         assertEquals(0l, jsonMessageParser.extractTimestampMillis(mMessageWithoutTimestamp));
     }
 
+    @Test
+    public void testExtractNestedTimestampMillis() throws Exception {
+        Mockito.when(mConfig.getMessageTimestampNameSeparator()).thenReturn(".");
+        Mockito.when(mConfig.getMessageTimestampName()).thenReturn("meta_data.created");
+
+        JsonMessageParser jsonMessageParser = new JsonMessageParser(mConfig);
+        assertEquals(1405911096123l, jsonMessageParser.extractTimestampMillis(mMessageWithNestedTimestamp));
+    }
+
     @Test(expected=ClassCastException.class)
     public void testExtractTimestampMillisException1() throws Exception {
         JsonMessageParser jsonMessageParser = new JsonMessageParser(mConfig);
diff --git a/src/test/java/com/pinterest/secor/performance/PerformanceTest.java b/src/test/java/com/pinterest/secor/performance/PerformanceTest.java
deleted file mode 100644
index 59dd1e4349d0b35adbdf9495ecaeb72559f54317..0000000000000000000000000000000000000000
--- a/src/test/java/com/pinterest/secor/performance/PerformanceTest.java
+++ /dev/null
@@ -1,272 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.pinterest.secor.performance;
-
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Random;
-
-import org.I0Itec.zkclient.ZkClient;
-
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import com.pinterest.secor.common.KafkaClient;
-import com.pinterest.secor.common.OffsetTracker;
-import com.pinterest.secor.common.OstrichAdminService;
-import com.pinterest.secor.common.SecorConfig;
-import com.pinterest.secor.common.TopicPartition;
-import com.pinterest.secor.consumer.Consumer;
-import com.pinterest.secor.tools.LogFileDeleter;
-import com.pinterest.secor.util.FileUtil;
-import com.pinterest.secor.util.RateLimitUtil;
-
-import kafka.admin.AdminUtils;
-import kafka.api.PartitionOffsetRequestInfo;
-import kafka.common.TopicAndPartition;
-import kafka.common.TopicExistsException;
-import kafka.javaapi.OffsetResponse;
-import kafka.javaapi.consumer.SimpleConsumer;
-import kafka.javaapi.producer.Producer;
-import kafka.producer.KeyedMessage;
-import kafka.producer.ProducerConfig;
-import kafka.utils.ZKStringSerializer$;
-
-/**
- * A performance test for secor
- * 
- * * Run: $ cd optimus/secor $ mvn package $ cd target $ java -ea
- * -Dlog4j.configuration=log4j.dev.properties
- * -Dconfig=secor.test.perf.backup.properties \ -cp
- * "secor-0.1-SNAPSHOT-tests.jar:lib/*:secor-0.1-SNAPSHOT.jar"
- * com.pinterest.secor.performance.PerformanceTest <num_topics> <num_partitions>
- * <num_records> <message_size>
- * 
- * @author Praveen Murugesan(praveen@uber.com)
- *
- */
-public class PerformanceTest {
-
-    public static void main(String[] args) throws Exception {
-        if (args.length != 4) {
-            System.err.println("USAGE: java " + PerformanceTest.class.getName()
-                    + " num_topics num_partitions num_records message_size");
-            System.exit(1);
-        }
-        Random rnd = new Random();
-        int num_topics = Integer.parseInt(args[0]);
-        SecorConfig config = SecorConfig.load();
-        String zkConfig = config.getZookeeperQuorum()
-                + config.getKafkaZookeeperPath();
-        // create topics list
-        String perfTopicPrefix = config.getPerfTestTopicPrefix();
-        List<String> topics = Lists.newLinkedList();
-        for (int i = 0; i < num_topics; i++) {
-            topics.add(perfTopicPrefix + rnd.nextInt(9999));
-        }
-
-        int num_partitions = Integer.parseInt(args[1]);
-
-        // createTopics
-        createTopics(topics, num_partitions, zkConfig);
-
-        int numRecords = Integer.parseInt(args[2]);
-        Properties props = new Properties();
-        props.put("metadata.broker.list", "localhost:9092");
-        props.put("serializer.class", "kafka.serializer.StringEncoder");
-        props.put("request.required.acks", "1");
-        props.put("producer.type", "async");
-
-        ProducerConfig producerConfig = new ProducerConfig(props);
-
-        Producer<String, String> producer = new Producer<String, String>(
-                producerConfig);
-        long size = 0;
-        int message_size = Integer.parseInt(args[3]);
-
-        // produce messages
-        for (String topic : topics) {
-            for (long nEvents = 0; nEvents < numRecords; nEvents++) {
-                String ip = String.valueOf(nEvents % num_partitions);
-                byte[] payload = new byte[message_size];
-                Arrays.fill(payload, (byte) 1);
-                String msg = new String(payload, "UTF-8");
-                size += msg.length();
-                KeyedMessage<String, String> data = new KeyedMessage<String, String>(
-                        topic, ip, msg);
-                producer.send(data);
-            }
-        }
-        producer.close();
-
-        RateLimitUtil.configure(config);
-        Map<TopicPartition, Long> lastOffsets = getTopicMetadata(topics,
-                num_partitions, config);
-        OstrichAdminService ostrichService = new OstrichAdminService(
-                config.getOstrichPort());
-        ostrichService.start();
-        FileUtil.configure(config);
-
-        LogFileDeleter logFileDeleter = new LogFileDeleter(config);
-        logFileDeleter.deleteOldLogs();
-        Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {
-            public void uncaughtException(Thread thread, Throwable exception) {
-                exception.printStackTrace();
-                System.out.println("Thread " + thread + " failed:"
-                        + exception.getMessage());
-                System.exit(1);
-            }
-        };
-        System.out.println("starting " + config.getConsumerThreads()
-                + " consumer threads");
-        System.out.println("Rate limit:" + config.getMessagesPerSecond());
-        LinkedList<Consumer> consumers = new LinkedList<Consumer>();
-        long startMillis = System.currentTimeMillis();
-        for (int i = 0; i < config.getConsumerThreads(); ++i) {
-            Consumer consumer = new Consumer(config);
-            consumer.setUncaughtExceptionHandler(handler);
-            consumers.add(consumer);
-            consumer.start();
-        }
-
-        while (true) {
-            for (Consumer consumer : consumers) {
-                for (String topic : topics) {
-                    for (int i = 0; i < num_partitions; i++) {
-                        OffsetTracker offsetTracker = consumer
-                                .getOffsetTracker();
-                        long val = (offsetTracker == null) ? -1
-                                : offsetTracker
-                                        .getLastSeenOffset(new TopicPartition(
-                                                topic, i)) + 1;
-
-                        System.out.println("topic:" + topic + " partition:" + i
-                                + " secor offset:" + val + " elapsed:"
-                                + (System.currentTimeMillis() - startMillis));
-                        Long lastOffset = lastOffsets.get(new TopicPartition(
-                                topic, i));
-                        if (lastOffset != null && lastOffset == val) {
-                            lastOffsets.remove(new TopicPartition(topic, i));
-                        }
-                    }
-                }
-            }
-
-            // time break to measure
-            Thread.sleep(1000);
-            System.out.println("last offsets size:" + lastOffsets.size());
-            if (lastOffsets.isEmpty()) {
-                long endMillis = System.currentTimeMillis();
-                System.out.println("Completed in:" + (endMillis - startMillis));
-                System.out.println("Total bytes:" + size);
-                // wait for the last file to be written
-                Thread.sleep(60000);
-                break;
-            }
-        }
-
-        System.exit(1);
-    }
-
-    /**
-     * Get topic partition to last offset map
-     * 
-     * @param topics
-     * @param num_partitions
-     * @param config
-     * @return
-     */
-    private static Map<TopicPartition, Long> getTopicMetadata(
-            List<String> topics, int num_partitions, SecorConfig config) {
-        KafkaClient mKafkaClient = new KafkaClient(config);
-
-        Map<TopicPartition, Long> lastOffsets = Maps.newHashMap();
-        for (String topic : topics) {
-            for (int i = 0; i < num_partitions; i++) {
-                TopicAndPartition topicAndPartition = new TopicAndPartition(
-                        topic, i);
-                SimpleConsumer consumer = mKafkaClient
-                        .createConsumer(new TopicPartition(topic, i));
-                Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
-                requestInfo.put(topicAndPartition,
-                        new PartitionOffsetRequestInfo(-1, 1));
-                kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(
-                        requestInfo, kafka.api.OffsetRequest.CurrentVersion(),
-                        "TestPerformance");
-                OffsetResponse response = consumer.getOffsetsBefore(request);
-                if (response.hasError()) {
-                    System.out
-                            .println("Error fetching data Offset Data the Broker. Reason: "
-                                    + response.errorCode(topic, i));
-                    return null;
-                }
-                long[] offsets = response.offsets(topic, i);
-                System.out.println("Topic: " + topic + " partition: " + i
-                        + " offset: " + offsets[0]);
-                lastOffsets.put(new TopicPartition(topic, i), offsets[0]);
-            }
-        }
-        return lastOffsets;
-    }
-
-    /**
-     * Helper to create topics
-     * 
-     * @param topics
-     * @param partitions
-     * @param zkConfig
-     * @throws InterruptedException
-     */
-    private static void createTopics(List<String> topics, int partitions,
-            String zkConfig) throws InterruptedException {
-
-        ZkClient zkClient = createZkClient(zkConfig);
-
-        try {
-            Properties props = new Properties();
-            int replicationFactor = 1;
-            for (String topic : topics) {
-                AdminUtils.createTopic(zkClient, topic, partitions,
-                        replicationFactor, props);
-            }
-        } catch (TopicExistsException e) {
-            System.out.println(e.getMessage());
-        } finally {
-            zkClient.close();
-        }
-
-    }
-
-    /**
-     * Helper to create ZK client
-     * 
-     * @param zkConfig
-     * @return
-     */
-    private static ZkClient createZkClient(String zkConfig) {
-        // Create a ZooKeeper client
-        int sessionTimeoutMs = 10000;
-        int connectionTimeoutMs = 10000;
-        ZkClient zkClient = new ZkClient(zkConfig, sessionTimeoutMs,
-                connectionTimeoutMs, ZKStringSerializer$.MODULE$);
-        return zkClient;
-    }
-
-}
\ No newline at end of file
diff --git a/src/test/java/com/pinterest/secor/supervisor/SupervisorTest.java b/src/test/java/com/pinterest/secor/supervisor/SupervisorTest.java
new file mode 100644
index 0000000000000000000000000000000000000000..24a772acff696baea1a5cad5fa2c9f33d4b294f3
--- /dev/null
+++ b/src/test/java/com/pinterest/secor/supervisor/SupervisorTest.java
@@ -0,0 +1,77 @@
+package com.pinterest.secor.supervisor;
+
+import com.pinterest.secor.common.SecorConfig;
+import com.pinterest.secor.common.SecorReports;
+import com.pinterest.secor.consumer.Consumer;
+import junit.framework.TestCase;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.TopicPartition;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.Mockito;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+
+import static org.mockito.Mockito.verify;
+
+/**
+ * Created by guy on 8/22/16.
+ */
+@RunWith(PowerMockRunner.class)
+public class SupervisorTest extends TestCase {
+    private SecorConfig mConfig;
+
+    @Override
+    public void setUp() throws Exception {
+        mConfig = Mockito.mock(SecorConfig.class);
+    }
+
+
+    // simple basic test
+    @Test
+    public void catchHangs() throws Exception {
+        final int forRuns = 4;
+        List<Consumer> consumers = new ArrayList<Consumer>();
+        Consumer liveConsumerMock = Mockito.mock(Consumer.class);
+        Consumer deadConsumerMock = Mockito.mock(Consumer.class);
+        Mockito.when(deadConsumerMock.getAssignedTP()).thenReturn(new HashSet<TopicPartition>());
+        Mockito.when(liveConsumerMock.getAssignedTP()).thenReturn(new HashSet<TopicPartition>());
+
+        Mockito.when(liveConsumerMock.getmReport()).then(new Answer<SecorReports>() {
+            @Override
+            public SecorReports answer(InvocationOnMock invocationOnMock) throws Throwable {
+                return new SecorReports(System.currentTimeMillis(), "status", true);
+            }
+        });
+
+        Mockito.when(deadConsumerMock.getmReport()).then(new Answer<SecorReports>() {
+            int counter = 0;
+            long time;
+            @Override
+            public SecorReports answer(InvocationOnMock invocationOnMock) throws Throwable {
+                if (counter++ < forRuns) {
+                    time = System.currentTimeMillis();
+                }
+                return new SecorReports(time, "START status", true);
+            }
+        });
+        consumers.add(deadConsumerMock);
+        consumers.add(liveConsumerMock);
+        Supervisor supervisor = new Supervisor(consumers, mConfig);
+        Throwable cause = null;
+        try {
+            supervisor.run();
+        } catch (RuntimeException e) {
+            cause = e.getCause();
+        }
+
+        assertEquals(cause.getMessage(), "consumer 0 hangs");
+    }
+}
\ No newline at end of file
diff --git a/src/test/java/com/pinterest/secor/uploader/UploaderTest.java b/src/test/java/com/pinterest/secor/uploader/UploaderTest.java
index 3e834595c43fe02437ed0a424054464dbedf9cd9..787d95a4621e8b8d42eadc6a9a79105669409392 100644
--- a/src/test/java/com/pinterest/secor/uploader/UploaderTest.java
+++ b/src/test/java/com/pinterest/secor/uploader/UploaderTest.java
@@ -33,10 +33,15 @@ import org.mockito.stubbing.Answer;
 import org.powermock.api.mockito.PowerMockito;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.OffsetAndMetadata;
+import org.apache.kafka.common.TopicPartition;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.HashSet;
 
+
 /**
  * UploaderTest tests the log file uploader logic.
  *
@@ -49,8 +54,8 @@ public class UploaderTest extends TestCase {
         private FileReader mReader;
 
         public TestUploader(SecorConfig config, OffsetTracker offsetTracker,
-                FileRegistry fileRegistry, ZookeeperConnector zookeeperConnector) {
-            super(config, offsetTracker, fileRegistry, zookeeperConnector);
+                FileRegistry fileRegistry, ZookeeperConnector zookeeperConnector, KafkaConsumer mKafkaConsumer) {
+            super(config, offsetTracker, fileRegistry, zookeeperConnector, mKafkaConsumer);
             mReader = Mockito.mock(FileReader.class);
         }
 
@@ -73,6 +78,8 @@ public class UploaderTest extends TestCase {
     private OffsetTracker mOffsetTracker;
     private FileRegistry mFileRegistry;
     private ZookeeperConnector mZookeeperConnector;
+    private KafkaConsumer mKafkaConsumer;
+    private OffsetAndMetadata offsetAndMetadata;
 
     private TestUploader mUploader;
 
@@ -86,6 +93,7 @@ public class UploaderTest extends TestCase {
                         + "10_0_00000000000000000010");
 
         mConfig = Mockito.mock(SecorConfig.class);
+        Mockito.when(mConfig.getCloudService()).thenReturn("Swift");
         Mockito.when(mConfig.getLocalPath()).thenReturn("/some_parent_dir");
         Mockito.when(mConfig.getMaxFileSizeBytes()).thenReturn(10L);
 
@@ -98,15 +106,19 @@ public class UploaderTest extends TestCase {
         Mockito.when(mFileRegistry.getTopicPartitions()).thenReturn(
                 topicPartitions);
 
+        mKafkaConsumer = Mockito.mock(KafkaConsumer.class);
+        Mockito.when(mKafkaConsumer.committed(mTopicPartition))
+                .thenReturn(offsetAndMetadata);
+        offsetAndMetadata = Mockito.mock(OffsetAndMetadata.class);
         mZookeeperConnector = Mockito.mock(ZookeeperConnector.class);
         mUploader = new TestUploader(mConfig, mOffsetTracker, mFileRegistry,
-                mZookeeperConnector);
+                mZookeeperConnector, mKafkaConsumer);
     }
 
     public void testUploadFiles() throws Exception {
-        Mockito.when(
-                mZookeeperConnector.getCommittedOffsetCount(mTopicPartition))
-                .thenReturn(11L);
+        Mockito.when(offsetAndMetadata.offset()).thenReturn(11L);
+        Mockito.when(mKafkaConsumer.committed(mTopicPartition))
+        .thenReturn(offsetAndMetadata);
         Mockito.when(
                 mOffsetTracker.setCommittedOffsetCount(mTopicPartition, 11L))
                 .thenReturn(11L);
@@ -118,8 +130,8 @@ public class UploaderTest extends TestCase {
         Mockito.when(
                 mOffsetTracker.getTrueCommittedOffsetCount(mTopicPartition))
                 .thenReturn(11L);
-        Mockito.when(mConfig.getS3Bucket()).thenReturn("some_bucket");
-        Mockito.when(mConfig.getS3Path()).thenReturn("some_s3_parent_dir");
+        Mockito.when(mConfig.getSwiftContainer()).thenReturn("some_container");
+        Mockito.when(mConfig.getSwiftPath()).thenReturn("some_swift_parent_dir");
 
         HashSet<LogFilePath> logFilePaths = new HashSet<LogFilePath>();
         logFilePaths.add(mLogFilePath);
@@ -133,23 +145,24 @@ public class UploaderTest extends TestCase {
         final String lockPath = "/secor/locks/some_topic/0";
         Mockito.verify(mZookeeperConnector).lock(lockPath);
         PowerMockito.verifyStatic();
-        FileUtil.moveToS3(
+        FileUtil.moveToCloud(
                 "/some_parent_dir/some_topic/some_partition/some_other_partition/"
                         + "10_0_00000000000000000010",
-                "s3n://some_bucket/some_s3_parent_dir/some_topic/some_partition/"
+                "swift2d://some_container.GENERICPROJECT/some_swift_parent_dir/some_topic/some_partition/"
                         + "some_other_partition/10_0_00000000000000000010");
         Mockito.verify(mFileRegistry).deleteTopicPartition(mTopicPartition);
-        Mockito.verify(mZookeeperConnector).setCommittedOffsetCount(
-                mTopicPartition, 21L);
+        HashMap<TopicPartition, OffsetAndMetadata> committed = new HashMap<TopicPartition, OffsetAndMetadata>();
+        committed.put(mTopicPartition,new OffsetAndMetadata(21L));
+        Mockito.verify(mKafkaConsumer).commitSync(committed);
         Mockito.verify(mOffsetTracker).setCommittedOffsetCount(mTopicPartition,
                 21L);
         Mockito.verify(mZookeeperConnector).unlock(lockPath);
     }
 
     public void testDeleteTopicPartition() throws Exception {
-        Mockito.when(
-                mZookeeperConnector.getCommittedOffsetCount(mTopicPartition))
-                .thenReturn(31L);
+        Mockito.when(offsetAndMetadata.offset()).thenReturn(31L);
+        Mockito.when(mKafkaConsumer.committed(mTopicPartition))
+        .thenReturn(offsetAndMetadata);
         Mockito.when(
                 mOffsetTracker.setCommittedOffsetCount(mTopicPartition, 30L))
                 .thenReturn(11L);
@@ -162,9 +175,9 @@ public class UploaderTest extends TestCase {
     }
 
     public void testTrimFiles() throws Exception {
-        Mockito.when(
-                mZookeeperConnector.getCommittedOffsetCount(mTopicPartition))
-                .thenReturn(21L);
+        Mockito.when(offsetAndMetadata.offset()).thenReturn(21L);
+        Mockito.when(mKafkaConsumer.committed(mTopicPartition))
+        .thenReturn(offsetAndMetadata);
         Mockito.when(
                 mOffsetTracker.setCommittedOffsetCount(mTopicPartition, 21L))
                 .thenReturn(20L);
diff --git a/src/test/java/com/pinterest/secor/util/ReflectionUtilTest.java b/src/test/java/com/pinterest/secor/util/ReflectionUtilTest.java
index 553b58ef98e91feb8e409b94bb23ebc006d5b505..0ab838a4bc266208db1f114bbca2caf971bdcb73 100644
--- a/src/test/java/com/pinterest/secor/util/ReflectionUtilTest.java
+++ b/src/test/java/com/pinterest/secor/util/ReflectionUtilTest.java
@@ -21,14 +21,18 @@ import com.pinterest.secor.common.SecorConfig;
 import com.pinterest.secor.parser.MessageParser;
 import org.apache.commons.configuration.PropertiesConfiguration;
 import org.junit.Test;
+import org.junit.Before;
 
 public class ReflectionUtilTest {
 
     private SecorConfig mSecorConfig;
     private LogFilePath mLogFilePath;
 
+    @Before
     public void setUp() throws Exception {
         PropertiesConfiguration properties = new PropertiesConfiguration();
+        properties.addProperty("message.timestamp.name","");
+        properties.addProperty("message.timestamp.name.separator","");
         mSecorConfig = new SecorConfig(properties);
         mLogFilePath = new LogFilePath("/foo", "/foo/bar/baz/1_1_1");
     }
@@ -64,4 +68,4 @@ public class ReflectionUtilTest {
         ReflectionUtil.createFileWriter("java.lang.Object",
                 mLogFilePath, null);
     }
-}
\ No newline at end of file
+}
diff --git a/test.schema b/test.schema
new file mode 100644
index 0000000000000000000000000000000000000000..5c6e5ef09cb51178caf1ec950d001359295f29fc
--- /dev/null
+++ b/test.schema
@@ -0,0 +1,9 @@
+{"namespace": "com.ibm",
+ "type": "record",
+ "name": "MessageHub",
+ "fields": [
+            {"name": "timestamp", "type": "long" },
+            {"name": "requiredField", "type": "string"},
+            {"name": "enumField", "type": "long" }
+           ]
+}  
